{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934206b0-59c0-4c54-b1f9-2422abd0b58a",
   "metadata": {},
   "source": [
    "## 기초 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a9a0829b-276c-4d4b-9c4a-aad73dd18eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dim = 512\n",
    "n_heads = 8\n",
    "\n",
    "dim_per_head = dim // n_heads\n",
    "\n",
    "print(dim_per_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5bd19df3-bdde-4a4a-b481-83829181c799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14., 32.],\n",
      "        [32., 77.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "B = torch.Tensor([\n",
    "    [1, 4], \n",
    "    [2, 5],\n",
    "    [3, 6]\n",
    "])\n",
    "C = torch.matmul(A, B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ad07b22-52ac-4e8a-bf51-2e98039f2838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0697, 0.7858, 0.5453,  ..., 0.6775, 0.4480, 0.6910],\n",
      "          [0.2766, 0.0698, 0.4162,  ..., 0.1777, 0.8022, 0.3807],\n",
      "          [0.5000, 0.7420, 0.9270,  ..., 0.9319, 0.3514, 0.9430],\n",
      "          [0.3707, 0.3878, 0.5769,  ..., 0.4808, 0.9184, 0.0537],\n",
      "          [0.0121, 0.0971, 0.2607,  ..., 0.8670, 0.4894, 0.2651]],\n",
      "\n",
      "         [[0.0725, 0.6900, 0.1731,  ..., 0.4663, 0.8738, 0.1976],\n",
      "          [0.0947, 0.8805, 0.0407,  ..., 0.6522, 0.6374, 0.1722],\n",
      "          [0.3488, 0.7712, 0.9439,  ..., 0.7995, 0.6770, 0.7722],\n",
      "          [0.1152, 0.0839, 0.3617,  ..., 0.3688, 0.2944, 0.2753],\n",
      "          [0.5751, 0.7502, 0.0508,  ..., 0.3478, 0.1272, 0.2526]],\n",
      "\n",
      "         [[0.6463, 0.3965, 0.9159,  ..., 0.3100, 0.7720, 0.0145],\n",
      "          [0.2610, 0.3123, 0.7381,  ..., 0.2167, 0.9840, 0.7408],\n",
      "          [0.9107, 0.2784, 0.8956,  ..., 0.6077, 0.8119, 0.0838],\n",
      "          [0.6687, 0.6608, 0.5911,  ..., 0.7178, 0.1452, 0.9359],\n",
      "          [0.4970, 0.0737, 0.5581,  ..., 0.9836, 0.5183, 0.9118]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1278, 0.3507, 0.8232,  ..., 0.2293, 0.8374, 0.2945],\n",
      "          [0.4812, 0.9378, 0.3099,  ..., 0.2289, 0.7608, 0.4640],\n",
      "          [0.2442, 0.8695, 0.0017,  ..., 0.9228, 0.4140, 0.9104],\n",
      "          [0.2389, 0.0093, 0.7134,  ..., 0.9319, 0.1829, 0.1502],\n",
      "          [0.3818, 0.4517, 0.2044,  ..., 0.7776, 0.2584, 0.4654]],\n",
      "\n",
      "         [[0.6105, 0.8160, 0.4350,  ..., 0.3322, 0.1744, 0.3985],\n",
      "          [0.9930, 0.2853, 0.9247,  ..., 0.0354, 0.0981, 0.7667],\n",
      "          [0.9876, 0.1206, 0.3942,  ..., 0.7209, 0.8150, 0.0067],\n",
      "          [0.9671, 0.7443, 0.7871,  ..., 0.0816, 0.3440, 0.3101],\n",
      "          [0.0210, 0.6304, 0.2710,  ..., 0.6311, 0.8377, 0.8443]],\n",
      "\n",
      "         [[0.1849, 0.4339, 0.1885,  ..., 0.6357, 0.4276, 0.3292],\n",
      "          [0.2438, 0.1588, 0.2869,  ..., 0.1855, 0.2659, 0.7878],\n",
      "          [0.2603, 0.5841, 0.1427,  ..., 0.4912, 0.0283, 0.0703],\n",
      "          [0.6751, 0.6996, 0.9484,  ..., 0.2132, 0.2284, 0.0441],\n",
      "          [0.4680, 0.5298, 0.0663,  ..., 0.7687, 0.2267, 0.5787]]]])\n",
      "torch.Size([1, 8, 5, 64])\n",
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "bs = 1\n",
    "seq_length = 5\n",
    "\n",
    "def shape(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"separate heads\"\"\"\n",
    "    return x.view(bs, -1, n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "def unshape(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"group heads\"\"\"\n",
    "    return x.transpose(1, 2).contiguous().view(bs, -1, n_heads * dim_per_head)\n",
    "\n",
    "query = torch.rand(bs, seq_length, dim)\n",
    "q = shape(query)\n",
    "print(q)\n",
    "print(q.size())\n",
    "\n",
    "q_ = unshape(q)\n",
    "print(q_.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b87a8-543a-4fe4-8947-bae7dbdf4062",
   "metadata": {},
   "source": [
    "## 멀티 헤드 어텐션(Multi-head Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8ef744b-438b-4573-8099-dba95effb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.n_heads = kwargs.pop(\"n_heads\", 8)\n",
    "        self.dim = kwargs.pop(\"dim\", 512)\n",
    "        self.attention_dropout = kwargs.pop(\"attention_dropout\", 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e99fda31-1cbf-4690-96c0-561596490a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "        self.is_causal = False\n",
    "\n",
    "        # Have an even number of multi heads that divide the dimensions\n",
    "        if self.dim % self.n_heads != 0:\n",
    "            # Raise value errors for even multi-head attention nodes\n",
    "            raise ValueError(f\"self.n_heads: {self.n_heads} must divide self.dim: {self.dim} evenly\")\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "        self.pruned_heads: Set[int] = set()\n",
    "        self.attention_head_size = self.dim // self.n_heads\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            query: torch.tensor(bs, seq_length, dim)\n",
    "            key: torch.tensor(bs, seq_length, dim)\n",
    "            value: torch.tensor(bs, seq_length, dim)\n",
    "            mask: torch.tensor(bs, seq_length)\n",
    "\n",
    "        Returns:\n",
    "            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n",
    "            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n",
    "        \"\"\"\n",
    "        bs, q_length, dim = query.size()\n",
    "        print(f\"bs: {bs}, q_length: {q_length}, dim: {dim}\")\n",
    "        k_length = key.size(1)\n",
    "        print(f\"k_length: {k_length}\")\n",
    "        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n",
    "        # assert key.size() == value.size()\n",
    "\n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "        print(f\"dim_per_head: {dim_per_head}\")\n",
    "\n",
    "        mask_reshp = (bs, 1, 1, k_length)\n",
    "        print(f\"mask_reshp: {mask_reshp}\")\n",
    "\n",
    "        def shape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"separate heads\"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"group heads\"\"\"\n",
    "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "\n",
    "        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)\n",
    "        print(\"=\" * 80)\n",
    "        print(mask)\n",
    "        print(\"-\" * 80)\n",
    "        scores = scores.masked_fill(\n",
    "            mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
    "        )  # (bs, n_heads, q_length, k_length)\n",
    "        print(\">\" * 80)\n",
    "        print(scores)\n",
    "        print(\"<\" * 80)\n",
    "\n",
    "        weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)\n",
    "        print(\"#\" * 80)\n",
    "        print(weights)\n",
    "        print(\"@\" * 80)\n",
    "        weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            weights = weights * head_mask\n",
    "\n",
    "        context = torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        context = unshape(context)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        if output_attentions:\n",
    "            return (context, weights)\n",
    "        else:\n",
    "            return (context,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b9e970a8-9f18-4efe-a729-66d88159ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PretrainedConfig(n_heads=8, dim=512, attention_dropout=0.2)\n",
    "mh_attention = MultiHeadSelfAttention(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d77d9d7a-a5eb-42c4-bf81-16f9dcb1b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n",
      "tensor([[[0.8823, 0.9150, 0.3829,  ..., 0.4078, 0.5411, 0.0410],\n",
      "         [0.6556, 0.1186, 0.1836,  ..., 0.3092, 0.0702, 0.1836],\n",
      "         [0.7785, 0.4253, 0.7124,  ..., 0.4593, 0.4520, 0.1866],\n",
      "         [0.5729, 0.3465, 0.2419,  ..., 0.5122, 0.5909, 0.9712],\n",
      "         [0.7322, 0.6075, 0.3988,  ..., 0.7021, 0.7154, 0.4832]]])\n",
      "tensor([1., 1., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "bs = 1\n",
    "seq_length = 5\n",
    "dim = 512\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "query = torch.rand(bs, seq_length, dim)\n",
    "key = torch.rand(bs, seq_length, dim)\n",
    "value = torch.rand(bs, seq_length, dim)\n",
    "#mask = torch.where(torch.randn(bs, seq_length) > 0, 1, 0)\n",
    "mask = torch.Tensor([1, 1, 1, 1, 0])\n",
    "print(query.size())\n",
    "print(query)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74bb68ce-1e97-413c-af1b-ef34af19b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs: 1, q_length: 5, dim: 512\n",
      "k_length: 5\n",
      "dim_per_head: 64\n",
      "mask_reshp: (1, 1, 1, 5)\n",
      "================================================================================\n",
      "tensor([[[[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]],\n",
      "\n",
      "         [[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]],\n",
      "\n",
      "         [[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]],\n",
      "\n",
      "         [[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]],\n",
      "\n",
      "         [[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]],\n",
      "\n",
      "         [[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]],\n",
      "\n",
      "         [[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]],\n",
      "\n",
      "         [[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]]]])\n",
      "--------------------------------------------------------------------------------\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "tensor([[[[-2.6333e-02, -2.5347e-02, -1.0201e-01, -1.9438e-02, -3.4028e+38],\n",
      "          [-9.4338e-03, -3.8450e-04, -8.0131e-02,  2.2029e-02, -3.4028e+38],\n",
      "          [ 6.1085e-02,  1.0373e-01, -8.3101e-02,  7.9367e-02, -3.4028e+38],\n",
      "          [-2.2937e-02, -5.9164e-02, -8.9759e-02, -5.5411e-02, -3.4028e+38],\n",
      "          [ 6.2383e-02,  6.9387e-02, -4.2842e-02,  6.2819e-02, -3.4028e+38]],\n",
      "\n",
      "         [[-2.1045e-03, -4.2860e-02,  2.1054e-02,  4.8208e-02, -3.4028e+38],\n",
      "          [ 3.8600e-02,  1.9946e-02,  1.1245e-01,  1.0600e-01, -3.4028e+38],\n",
      "          [ 1.2555e-01,  1.2947e-01,  1.9544e-01,  2.1329e-01, -3.4028e+38],\n",
      "          [-9.1624e-03, -9.8650e-04, -1.3707e-03,  1.1130e-02, -3.4028e+38],\n",
      "          [ 3.3551e-02,  6.8140e-02,  6.8651e-02,  6.8043e-02, -3.4028e+38]],\n",
      "\n",
      "         [[ 1.6203e-01,  6.2462e-02,  4.8490e-02,  9.0607e-02, -3.4028e+38],\n",
      "          [ 2.0279e-01,  6.8901e-02,  7.8294e-03,  1.2091e-01, -3.4028e+38],\n",
      "          [ 7.7373e-02,  9.7121e-03, -2.2874e-02,  8.6422e-02, -3.4028e+38],\n",
      "          [ 9.2815e-02, -4.0624e-03, -1.2126e-02,  1.3643e-02, -3.4028e+38],\n",
      "          [ 1.4053e-01,  8.6518e-02,  4.7304e-02,  1.2304e-01, -3.4028e+38]],\n",
      "\n",
      "         [[ 1.3102e-01,  1.0850e-01,  1.3077e-01,  1.3438e-01, -3.4028e+38],\n",
      "          [ 1.6970e-01,  1.4169e-01,  1.4248e-01,  1.8462e-01, -3.4028e+38],\n",
      "          [ 1.6141e-01,  1.4304e-01,  1.2702e-01,  9.6330e-02, -3.4028e+38],\n",
      "          [ 1.5680e-01,  1.3442e-01,  1.4006e-01,  1.4031e-01, -3.4028e+38],\n",
      "          [ 2.0381e-01,  1.7671e-01,  2.2848e-01,  1.7916e-01, -3.4028e+38]],\n",
      "\n",
      "         [[-2.4366e-01, -8.6730e-02, -3.1802e-02, -2.3745e-01, -3.4028e+38],\n",
      "          [-2.2734e-01, -1.5434e-01, -1.0502e-01, -1.8974e-01, -3.4028e+38],\n",
      "          [-2.8107e-01, -1.7868e-01, -1.2320e-01, -2.9134e-01, -3.4028e+38],\n",
      "          [-2.0154e-01, -1.2258e-01, -6.1741e-03, -2.0336e-01, -3.4028e+38],\n",
      "          [-1.3807e-01, -1.4405e-02,  3.5073e-02, -6.3813e-02, -3.4028e+38]],\n",
      "\n",
      "         [[ 1.0731e-01,  2.1462e-01,  1.4545e-01,  1.7221e-01, -3.4028e+38],\n",
      "          [ 1.7161e-01,  2.6245e-01,  1.8487e-01,  2.0037e-01, -3.4028e+38],\n",
      "          [ 1.9481e-01,  2.9529e-01,  1.7817e-01,  2.1526e-01, -3.4028e+38],\n",
      "          [ 2.1846e-01,  2.9055e-01,  2.2758e-01,  2.7809e-01, -3.4028e+38],\n",
      "          [ 2.2780e-01,  3.0524e-01,  2.4219e-01,  2.6468e-01, -3.4028e+38]],\n",
      "\n",
      "         [[-8.9876e-02, -7.2001e-02, -8.5856e-02, -5.3673e-02, -3.4028e+38],\n",
      "          [-9.6252e-02, -9.5251e-02, -6.0962e-02, -5.6987e-02, -3.4028e+38],\n",
      "          [-1.4122e-02, -2.6751e-02, -1.8442e-02, -2.6682e-02, -3.4028e+38],\n",
      "          [-1.4882e-02, -1.0896e-01, -1.6505e-02, -2.7080e-03, -3.4028e+38],\n",
      "          [-7.0592e-02, -8.9178e-02, -1.8665e-01, -7.0109e-02, -3.4028e+38]],\n",
      "\n",
      "         [[-8.3699e-02, -9.1270e-02, -4.9505e-02, -1.1875e-02, -3.4028e+38],\n",
      "          [-5.6169e-02, -1.2458e-01, -5.9045e-02,  1.5801e-02, -3.4028e+38],\n",
      "          [-1.5055e-01, -1.5440e-01, -1.2699e-01, -1.3239e-01, -3.4028e+38],\n",
      "          [-1.3328e-01, -1.4465e-01, -1.3489e-01, -1.0675e-01, -3.4028e+38],\n",
      "          [-4.6369e-02, -4.6722e-02,  8.0518e-03, -9.7702e-03, -3.4028e+38]]]],\n",
      "       grad_fn=<MaskedFillBackward1>)\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "################################################################################\n",
      "tensor([[[[0.2541, 0.2544, 0.2356, 0.2559, 0.0000],\n",
      "          [0.2517, 0.2540, 0.2345, 0.2598, 0.0000],\n",
      "          [0.2546, 0.2657, 0.2204, 0.2593, 0.0000],\n",
      "          [0.2585, 0.2493, 0.2418, 0.2503, 0.0000],\n",
      "          [0.2559, 0.2577, 0.2304, 0.2560, 0.0000]],\n",
      "\n",
      "         [[0.2478, 0.2379, 0.2536, 0.2606, 0.0000],\n",
      "          [0.2423, 0.2378, 0.2608, 0.2591, 0.0000],\n",
      "          [0.2399, 0.2409, 0.2573, 0.2619, 0.0000],\n",
      "          [0.2477, 0.2498, 0.2497, 0.2528, 0.0000],\n",
      "          [0.2435, 0.2521, 0.2522, 0.2521, 0.0000]],\n",
      "\n",
      "         [[0.2682, 0.2428, 0.2394, 0.2497, 0.0000],\n",
      "          [0.2763, 0.2417, 0.2274, 0.2546, 0.0000],\n",
      "          [0.2599, 0.2429, 0.2351, 0.2622, 0.0000],\n",
      "          [0.2680, 0.2432, 0.2413, 0.2476, 0.0000],\n",
      "          [0.2603, 0.2467, 0.2372, 0.2558, 0.0000]],\n",
      "\n",
      "         [[0.2512, 0.2456, 0.2511, 0.2520, 0.0000],\n",
      "          [0.2525, 0.2455, 0.2457, 0.2563, 0.0000],\n",
      "          [0.2574, 0.2527, 0.2487, 0.2412, 0.0000],\n",
      "          [0.2535, 0.2479, 0.2493, 0.2493, 0.0000],\n",
      "          [0.2516, 0.2449, 0.2579, 0.2455, 0.0000]],\n",
      "\n",
      "         [[0.2266, 0.2652, 0.2801, 0.2281, 0.0000],\n",
      "          [0.2356, 0.2535, 0.2663, 0.2446, 0.0000],\n",
      "          [0.2343, 0.2595, 0.2743, 0.2319, 0.0000],\n",
      "          [0.2328, 0.2519, 0.2830, 0.2323, 0.0000],\n",
      "          [0.2274, 0.2573, 0.2704, 0.2449, 0.0000]],\n",
      "\n",
      "         [[0.2370, 0.2639, 0.2462, 0.2529, 0.0000],\n",
      "          [0.2417, 0.2647, 0.2449, 0.2487, 0.0000],\n",
      "          [0.2433, 0.2690, 0.2393, 0.2483, 0.0000],\n",
      "          [0.2412, 0.2593, 0.2434, 0.2561, 0.0000],\n",
      "          [0.2420, 0.2615, 0.2455, 0.2511, 0.0000]],\n",
      "\n",
      "         [[0.2464, 0.2508, 0.2474, 0.2555, 0.0000],\n",
      "          [0.2453, 0.2455, 0.2541, 0.2551, 0.0000],\n",
      "          [0.2518, 0.2487, 0.2508, 0.2487, 0.0000],\n",
      "          [0.2550, 0.2321, 0.2546, 0.2582, 0.0000],\n",
      "          [0.2582, 0.2535, 0.2299, 0.2584, 0.0000]],\n",
      "\n",
      "         [[0.2438, 0.2420, 0.2523, 0.2620, 0.0000],\n",
      "          [0.2496, 0.2331, 0.2489, 0.2683, 0.0000],\n",
      "          [0.2476, 0.2467, 0.2535, 0.2522, 0.0000],\n",
      "          [0.2491, 0.2463, 0.2487, 0.2558, 0.0000],\n",
      "          [0.2443, 0.2442, 0.2580, 0.2534, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "(tensor([[[-0.1715, -0.0611,  0.3554,  ...,  0.0471,  0.3928, -0.0518],\n",
      "         [-0.1361, -0.0825,  0.3528,  ...,  0.0596,  0.3948, -0.0229],\n",
      "         [-0.1339, -0.0791,  0.3357,  ...,  0.0581,  0.3413, -0.0923],\n",
      "         [-0.0412, -0.0179,  0.3030,  ...,  0.0753,  0.2916, -0.1254],\n",
      "         [-0.1462, -0.1265,  0.3108,  ..., -0.0683,  0.3285, -0.0173]]],\n",
      "       grad_fn=<ViewBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "context = mh_attention.forward(query, key, value, mask, None)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cbd59-2c2f-43f0-ba59-33d3b2e04c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
