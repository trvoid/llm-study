{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/trvoid/llm-study/blob/main/bert/getting_started_with_distilbert_for_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cvu_6JOiK0j"
   },
   "source": [
    "# DistilBERT for MLM 시작하기 (한국어)\n",
    "\n",
    "**노트: distilbert-base-multilingual-cased를 기본 모델로 사용하였음**\n",
    "\n",
    "이 실습은 아래 문서의 내용을 토대로 진행하였습니다.\n",
    "\n",
    "* [2. 마스크 언어 모델(Masked Language Model) 미세조정](https://wikidocs.net/166833), Transformers (신경망 언어모델 라이브러리) 강좌\n",
    "* [Fine-tuning a masked language model](https://huggingface.co/learn/llm-course/en/chapter7/3), LLM Course in Hugging Face\n",
    "\n",
    "사용할 모델과 데이터셋은 다음과 같습니다.\n",
    "\n",
    "* [DistilBERT](distilbert/distilbert-base-multilingual-cased):기본 모델 (단어에 대한 단순 임베딩이 아니라 맥락을 고려한 임베딩 수행)\n",
    "* [DistilBertForMaskedLM](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertForMaskedLM): DistilBERT 모델에 마스크 언어 모델 층을 추가한 것으로서 마스크 언어 모델 미세조정 훈련을 위한 모델\n",
    "* [KorQuAD 1.0](https://huggingface.co/datasets/KorQuAD/squad_kor_v1): 질의응답 미세조정 훈련을 위한 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이저 및 모델 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU 사용 가능 여부 확인 및 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:25:43.972208: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 16:25:43.980482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744183543.989090 2943846 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744183543.991702 2943846 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744183543.999729 2943846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744183543.999735 2943846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744183543.999736 2943846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744183543.999737 2943846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 16:25:44.002260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DistilBertForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = DistilBertForMaskedLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 119547\n",
      "model device: cuda:0\n",
      "DistilBERT number of parameters: 135M\n",
      "BERT number of parameters: 110M\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocab size: {len(tokenizer)}\")\n",
    "print(f\"model device: {next(model.parameters()).device}\")\n",
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"DistilBERT number of parameters: {round(distilbert_num_parameters)}M\")\n",
    "print(f\"BERT number of parameters: 110M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([   101,   9309,  24982, 118922,  80795,   9491,  38378,   9730,  44321,\n",
      "           123,  14646,  17730,  12638,  18589,   9379,  14871,  48387, 105197,\n",
      "         20626,  62672,   9485,  25549,    102])\n",
      "len(input_ids): 23\n",
      "token_strs: ['[CLS]', '미', '##세', '##먼', '##지가', '심', '##하면', '차', '##량', '2', '##부', '##제', '##와', '같은', '비', '##상', '##저', '##감', '##조', '##치를', '시', '##행', '[SEP]']\n",
      "decoded_text: [CLS] 미세먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행 [SEP]\n",
      "input_ids: tensor([   101,   9309,  24982, 118922,  80795,   9491,  38378,   9730,  44321,\n",
      "           123,  14646,  17730,  12638,    103,   9379,  14871,  48387, 105197,\n",
      "         20626,  62672,   9485,  25549,    102])\n",
      "len(input_ids): 23\n",
      "token_strs: ['[CLS]', '미', '##세', '##먼', '##지가', '심', '##하면', '차', '##량', '2', '##부', '##제', '##와', '[MASK]', '비', '##상', '##저', '##감', '##조', '##치를', '시', '##행', '[SEP]']\n",
      "decoded_text: [CLS] 미세먼지가 심하면 차량 2부제와 [MASK] 비상저감조치를 시행 [SEP]\n"
     ]
    }
   ],
   "source": [
    "def test_tokenizer(tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=14, padding=\"max_length\")\n",
    "    print(f\"input_ids: {inputs[\"input_ids\"][0]}\")\n",
    "    print(f\"len(input_ids): {len(inputs[\"input_ids\"][0])}\")\n",
    "    \n",
    "    token_strs = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    print(f\"token_strs: {token_strs}\")\n",
    "    \n",
    "    decoded_text = tokenizer.decode(inputs[\"input_ids\"][0])\n",
    "    print(f\"decoded_text: {decoded_text}\")\n",
    "\n",
    "org_text = \"미세먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행\"\n",
    "text = \"미세먼지가 심하면 차량 2부제와 [MASK] 비상저감조치를 시행\"\n",
    "\n",
    "test_tokenizer(tokenizer, org_text)\n",
    "test_tokenizer(tokenizer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> 미세먼지가 심하면 차량 2부제와 ##의 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 ##가 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 ##이 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 ##와 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 ##지 비상저감조치를 시행'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def find_topk_for_masked(tokenizer, model, text, topk=5):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "    token_logits = model(**inputs).logits\n",
    "    #print(token_logits.shape)\n",
    "    \n",
    "    # [MASK]의 위치를 찾고, 해당 logits을 추출합니다.\n",
    "    #print(torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id))\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    #print(mask_token_index)\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    #print(mask_token_logits)\n",
    "    \n",
    "    # 가장 큰 logits값을 가지는 [MASK] 후보를 선택합니다.\n",
    "    top_5_tokens = torch.topk(mask_token_logits, topk, dim=1).indices[0].tolist()\n",
    "    \n",
    "    return top_5_tokens\n",
    "\n",
    "topk_tokens = find_topk_for_masked(tokenizer, model, text, 5)\n",
    "for token in topk_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 60407\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 5774\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"KorQuAD/squad_kor_v1\"\n",
    "dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> context: 9월 26일 환경부를 비롯한 12개 관계부처가 합동으로 '미세먼지 관리 종합대책'을 확정하고 발전·산업·수송·생활 등 4개 부분에서 저감 대책을 실시하는 관련 로드맵을 발표했다. 7조 2000억 원의 예산을 투입해 미세먼지 국내 배출량을 30% 감축하고 미세먼지 '나쁨' 일수를 70%까지 줄이기로 한 것인데 이를 위해 공정률 10% 미만인 석탄발전소 9기 중 4기를 LNG 등 친환경 연료로 전환하고 남은 5기도 최고 수준의 배출 기준을 적용하며 30년이 넘은 노후 석탄발전소 7기는 임기 내 폐쇄하기로 했다. 또한 대기배출총량제를 전국으로 확대·강화하고 먼지총량제를 새로 도입하며, 노후 경유차 221만 대를 임기 내 77% 조기 폐차하고 친환경 차를 2022년까지 200만 대 보급하며 미세먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행하기로 했다. 국제적으로는 미세먼지를 한중 양국의 정상의제로 격상하고 동북아 지역에서 협약체결을 추진하면서 미세먼지 환경기준도 선진국 수준으로 강화할 것도 포함했다.'\n",
      "'>>> question: 미세먼지 해결을 위해 전국으로 확대 강화된 기존의 제도는?'\n",
      "\n",
      "'>>> context: 프리스틀리는 워링턴 거주 시절에는 다른 일 때문에 신학 연구에 몰두하지 못하였으나, 리즈에 오면서 그는 신학 연구에 많은 시간을 투자하였고, 결과적으로 그의 신앙은 아리우스주의에서 유니테리언으로 정립되었다. 리즈에서 프리스틀리는 삼위일체와 예수의 선재성(先在性, 성자인 예수는 천지창조전부터 성부와 같이 존재했다는 교리)등을 완전히 부정하였고, 기독교 교리와 성서를 새롭게 해석하기 시작했다. 그는 오래전부터 써오던 신학 교육에 대한 책인 《자연과 계시 종교의 원리》(Institutes of Natural and Revealed Religion)를 출판하기 시작하였는데, 1772년에 1권이 출판되었고 마지막 3권은 1774년에 출판되었다. 그는 책에서 자연 종교, 계시의 진실성을 뒷받침하는 논거, 계시로부터 얻을 수 있는 진실 등을 논했다. 이 책은 약 반세기간의 자유주의 신학자들의 주장을 요약하였고 오랜 기간에 걸쳐 유니테리언의 대표적인 해설서가 되었다.'\n",
      "'>>> question: 오랜 신학 연구 끝에 프리스틀리의 신앙은 아리우스주의를 거쳐 무엇으로 정립되었는가?'\n",
      "\n",
      "'>>> context: 그는 밤이 새도록 근로기준법 조문을 찾아 암기하며 현장에서 발생하는 노동자 불이익에 저항하였다. 그는 동대문구청을 찾아가 열악한 환경에 대해 호소했지만 받아들여지지 않았다. 그는 근로기준법상의 감독권 행사를 요청하기 위해 시청 근로감독관실을 찾아갔지만, 근로감독관은 평화 시장의 참혹한 얘기에 관심 조차 보이지 않았다. 그는 다시 노동청의 문을 두드렸지만 결과는 마찬가지였다. 가뜩이나 어려운 조건 속에서 발버둥치고 있던 전태일에게 그것은 너무나 큰 충격이었다. 노동이나 근로 기준법조차 사업주들의 편이라는 현실은 그를 한동안 허탈 상태로 몰아 넣었다. 그는 청계천 일대의 노동 실태를 직접 조사, 설문하여 이를 토대로 근로기준법 준수를 요구하는 청원서를 노동청에 냈으나 돌아온 답변은 경멸과 비웃음 뿐이었다. 처음에 약간 말투가 어눌했던 그는 부랑자로 몰리거나, 노동청 공무원들에게 조롱의 대상이 되기도 했다. 그의 노동자 권리 청원은 언론들도 외면했고, 경향신문 등에만 간략하게 보도되었다.'\n",
      "'>>> question: 그는 근로기준법상의 감독권 행사를 요청하려고 어느 곳을 찾아갔는가?'\n"
     ]
    }
   ],
   "source": [
    "samples = dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in samples:\n",
    "    print(f\"\\n'>>> context: {row['context']}'\")\n",
    "    print(f\"'>>> question: {row['question']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, max_length=512):\n",
    "    result = tokenizer(examples[\"question\"], \n",
    "                       examples[\"context\"],\n",
    "                       max_length=max_length, \n",
    "                       truncation=\"only_second\"\n",
    "                      )\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8888, 37114, 57362, 9546, 48446, 10530, 9522, 119081, 25503, 118671, 136, 102, 8888, 37114, 57362, 9637, 13764, 10530, 9522, 119081, 48345, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'word_ids': [[None, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, None, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, None]]}\n",
      "['[CLS]', '고', '##양', '##이가', '어', '##디', '##에', '앉', '##습', '##니', '##까', '?', '[SEP]', '고', '##양', '##이가', '의', '##자', '##에', '앉', '##습', '##니다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "test_examples = {\n",
    "    \"context\": [\"고양이가 의자에 앉습니다.\"],\n",
    "    \"question\": [\"고양이가 어디에 앉습니까?\"],\n",
    "    \"answers\": [\n",
    "        {\n",
    "            \"text\": [\"의자\"],\n",
    "            \"answer_start\": [5]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "test_result = tokenize_function(test_examples)\n",
    "print(test_result)\n",
    "token_strs = tokenizer.convert_ids_to_tokens(test_result[\"input_ids\"][0])\n",
    "print(token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 60407\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 5774\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 빠른 멀티스레딩을 작동시키기 위해서, batched=True를 지정합니다.\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_max_length: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"model_max_length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk_size = 128\n",
    "chunk_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 length: 429\n",
      "Review 1 length: 428\n",
      "Review 2 length: 428\n"
     ]
    }
   ],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"Review {idx} length: {len(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated reviews length: 1285\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"Concatenated reviews length: {total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 150'\n",
      "'>>> Chunk length: 150'\n",
      "'>>> Chunk length: 150'\n",
      "'>>> Chunk length: 150'\n",
      "'>>> Chunk length: 150'\n",
      "'>>> Chunk length: 150'\n",
      "'>>> Chunk length: 150'\n",
      "'>>> Chunk length: 150'\n",
      "'>>> Chunk length: 85'\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # 모든 텍스트들을 결합한다.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # 결합된 텍스트들에 대한 길이를 구한다.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # `chunk_size`보다 작은 경우 마지막 청크를 삭제\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # max_len 길이를 가지는 chunk 단위로 슬라이스\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # 새로운 레이블 컬럼을 생성\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 132044\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 12771\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "print(lm_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡 ('"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡 ('"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] 바그너는 괴 [MASK]의 파우스트를 [MASK]고 무엇을 쓰고자 했는 [MASK]? [SEP] 1839년 바 [MASK]너는 괴테의 파우스트을 처음 읽고 [MASK] 내용에 마음 règle 끌려 이를 [MASK]재 [MASK] 해서 [MASK] 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바 [MASK]너는 1838년에 빛 독촉으로 산전수전을 다 [UNK] 상황이라 좌절과 실망에 가득했으며 메피스토펠 [MASK]를 만나는 파우스트 [MASK] 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관 [MASK]악단이 연주하는 베토벤의'\n",
      "\n",
      "'>>> 교향 [MASK] [MASK]번을 [MASK]고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파 [MASK]트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리 [MASK] 것은 의심 [MASK] 여지가 없다. 여기의 라 [MASK]조 조성의 경우에도 그의 전기에 적혀 있는 [MASK]처럼 단순한 [MASK]신 [MASK] 피로나 실의가 [MASK]영된 것이 [MASK] 베토벤 [MASK] 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 [MASK] 파리에서 착수했으나 [MASK]악 [MASK] 쓴 뒤에 중단했다. 또한 작 [MASK]의 [MASK]성과 동시에 그는 이 서곡 ('\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # 단어와 해당 토큰 인덱스 간의 map 생성\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # 무작위로 단어 마스킹\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] [MASK] [MASK] [MASK] [MASK] 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가? [SEP] 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다 [MASK] 이 시기 바그너는 [MASK] [MASK] 빛 독촉으로 산전수전을 다 [UNK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] 실망에 [MASK] [MASK] [MASK] 메피스토펠레스를 만나는 [MASK] [MASK] [MASK] [MASK] 심경에 공감했다고 한다 [MASK] 또한 파리에서 아브네크의 지휘로 파리 [MASK] [MASK] 관현악단이 연주하는 베토벤의'\n",
      "\n",
      "'>>> 교향곡 9번을 [MASK] [MASK] 깊은 [MASK] [MASK] 받았는데, 이것이 이듬해 1월에 파우스트의 [MASK] [MASK] [MASK] 쓰여진 이 작품에 조금이라도 [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 [MASK] 것처럼 단순한 정신적 피로나 실의가 반영된 [MASK] [MASK] 베토벤의 합창교향곡 조성의 [MASK] 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 [MASK] [MASK] [MASK] 쓴 [MASK] [MASK] 중단했다 [MASK] 또한 작품의 완성과 동시에 그는 이 서곡 [MASK]'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터 개수와 테스트 데이터 개수를 지정하여 샘플링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 118839\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 13205\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_size = 10_000\n",
    "#test_size = int(0.1 * train_size)\n",
    "train_size = None\n",
    "test_size = 0.1\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미세조정 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjeong/DevEnv/py312/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-korquad\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=4.0,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='414' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/207 05:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 67.94\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7428' max='7428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7428/7428 19:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.599800</td>\n",
       "      <td>2.192273</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.258800</td>\n",
       "      <td>2.054537</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.149700</td>\n",
       "      <td>1.996299</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.103300</td>\n",
       "      <td>1.967951</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "finetuned_model_path = \"./fine-tuned-distilbert-korquad-mlm\"\n",
    "tokenizer.save_pretrained(finetuned_model_path)\n",
    "model.save_pretrained(finetuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/207 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 7.11\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'input text: 미세먼지가 심하면 차량 2부제와 [MASK] 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 함께 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 동시에 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 같이 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 달리 비상저감조치를 시행'\n",
      "'input text: 미[MASK]먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##리먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##치는먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##치먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##의먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##터먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'input text: 미세먼지가 심[MASK] 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##한 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##해 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##각 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##하게 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##리 차량 2부제와 같은 비상저감조치를 시행'\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"미세먼지가 심하면 차량 2부제와 [MASK] 비상저감조치를 시행\", \n",
    "    \"미[MASK]먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행\",\n",
    "    \"미세먼지가 심[MASK] 차량 2부제와 같은 비상저감조치를 시행\"\n",
    "]\n",
    "for text in test_texts:\n",
    "    print(f\"'input text: {text}'\")\n",
    "    topk_tokens = find_topk_for_masked(tokenizer, model, text, topk=5)\n",
    "    for token in topk_tokens:\n",
    "        print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZ3y78jzXbpSmLBC0mDN3f",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ade48e3bc6d4a46a86b599e87ea406b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30d72eedc6714e21a7f200cb0abcfd33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32fa3ff8af104c4d9f23ed652c721e20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de2a2c502be4e4fa4770ea0eb4fe94e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8eabe883b07b4d968dd2cd8bff9ca264",
      "placeholder": "​",
      "style": "IPY_MODEL_6de4057a54f84d10a3dcc7e0535a86e5",
      "value": " 10570/10570 [00:15&lt;00:00, 788.93 examples/s]"
     }
    },
    "6de4057a54f84d10a3dcc7e0535a86e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85e055a4dcaf4131ad4cad24b5b2410c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87df2842719045488595c9725e386297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed9c813e80fc43b096bda875f665b176",
      "max": 10570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ade48e3bc6d4a46a86b599e87ea406b",
      "value": 10570
     }
    },
    "8eabe883b07b4d968dd2cd8bff9ca264": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4088c8d15fb41efb56ca8295c7f25b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32fa3ff8af104c4d9f23ed652c721e20",
      "placeholder": "​",
      "style": "IPY_MODEL_30d72eedc6714e21a7f200cb0abcfd33",
      "value": "Map: 100%"
     }
    },
    "d2345c97cf9e4f4a9c1a1344c928fdf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4088c8d15fb41efb56ca8295c7f25b5",
       "IPY_MODEL_87df2842719045488595c9725e386297",
       "IPY_MODEL_6de2a2c502be4e4fa4770ea0eb4fe94e"
      ],
      "layout": "IPY_MODEL_85e055a4dcaf4131ad4cad24b5b2410c"
     }
    },
    "ed9c813e80fc43b096bda875f665b176": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
