{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47113603-3b3b-4ecb-9232-2e7cef00b977",
   "metadata": {},
   "source": [
    "# DistilBERT + KorQuAD + MLM 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846992c1-10f3-4711-ab8c-9535904cdefb",
   "metadata": {},
   "source": [
    "## 1. 데이터셋 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb19c195-eeb9-4515-853c-3516d933a3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 60407\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 5774\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"KorQuAD/squad_kor_v1\"\n",
    "dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5aa801-93b4-4426-8b06-2947c2c829db",
   "metadata": {},
   "source": [
    "## 2. 토크나이저 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12c13b1-5769-47ee-a0d7-65069d7a01e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c722d040-6f0e-4a1a-9f82-8490e24566c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples, max_length=512):\n",
    "    result = tokenizer(examples[\"question\"], \n",
    "                       examples[\"context\"],\n",
    "                       max_length=max_length, \n",
    "                       truncation=\"only_second\"\n",
    "                      )\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b75414-b0ed-45b7-bf1d-69e2854f2ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 60407\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 5774\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 빠른 멀티스레딩을 작동시키기 위해서, batched=True를 지정합니다.\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5cfe0-ead3-4959-8027-7bf442ba694d",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c1d89b-157e-4bc7-a0e2-09b8e9f575a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#chunk_size = 128\n",
    "chunk_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b84b1212-3b37-4af9-99cb-90045a3d4e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # 모든 텍스트들을 결합한다.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # 결합된 텍스트들에 대한 길이를 구한다.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # `chunk_size`보다 작은 경우 마지막 청크를 삭제\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # max_len 길이를 가지는 chunk 단위로 슬라이스\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # 새로운 레이블 컬럼을 생성\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f1a73a-3470-4be6-b493-31dbeb1f208e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 132044\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 12771\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "print(lm_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b3fa772-085d-4f20-9c06-0aa91d8c9d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bddada9-c85b-4e4f-95d4-f16656555de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # 단어와 해당 토큰 인덱스 간의 map 생성\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # 무작위로 단어 마스킹\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9750ac94-8306-49d2-973d-825ce090fc56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 118839\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 13205\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#train_size = 10_000\n",
    "#test_size = int(0.1 * train_size)\n",
    "train_size = None\n",
    "test_size = 0.1\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "print(downsampled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7214d-3a1b-47d0-a13c-b7f1008c9c44",
   "metadata": {},
   "source": [
    "## 4. 모델 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb2f31c-dd12-46e8-b425-02495d74c616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU 사용 가능 여부 확인 및 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a43dc22-3f26-478f-863e-49a9edcfa970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForMaskedLM\n",
    "\n",
    "model = DistilBertForMaskedLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "353fd9c6-5c27-42a3-bf22-97344ae72292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 저장 테스트\n",
    "if False:\n",
    "    finetuned_model_path = \"./fine-tuned-distilbert-korquad-mlm\"\n",
    "    tokenizer.save_pretrained(finetuned_model_path)\n",
    "    model.save_pretrained(finetuned_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01048864-42b4-44f8-ad24-301170eb67aa",
   "metadata": {},
   "source": [
    "## 5. 미세조정 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d10bf841-96c9-4963-b715-b90ae73ab198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745207656.090026   49350 cuda_executor.cc:1004] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1745207656.092722   49203 service.cc:146] XLA service 0x55f1767f44a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745207656.092747   49203 service.cc:154]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1745207656.093174   49203 se_gpu_pjrt_client.cc:897] Using BFC allocator.\n",
      "I0000 00:00:1745207656.093230   49203 gpu_helpers.cc:114] XLA backend allocating 11731746816 bytes on device 0 for BFCAllocator.\n",
      "I0000 00:00:1745207656.093259   49203 gpu_helpers.cc:154] XLA backend will use up to 3910582272 bytes on device 0 for CollectiveBFCAllocator.\n",
      "I0000 00:00:1745207656.093416   49203 cuda_executor.cc:1004] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "/opt/python/3.10/lib/python3.10/site-packages/torch_xla/amp/grad_scaler.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#epochs = 4.0\n",
    "epochs = 0.2\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-korquad\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d37642c-fc4d-4f28-89b5-f6a275a1a392",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745207657.477111   49203 cuda_dnn.cc:530] Loaded cuDNN version 90100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 08:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.730509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=372, training_loss=3.0694957446026545, metrics={'train_runtime': 686.5626, 'train_samples_per_second': 34.621, 'train_steps_per_second': 0.542, 'total_flos': 926521620249600.0, 'train_loss': 3.0694957446026545, 'epoch': 0.20032310177705978})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d62bb-849a-40ce-9fc3-11d2e9fd62e7",
   "metadata": {},
   "source": [
    "## 6. 모델 저장 및 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a9f32d5-bba7-4c47-a286-50bfbf4818e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuned_model_path = \"./fine-tuned-distilbert-korquad-mlm\"\n",
    "tokenizer.save_pretrained(finetuned_model_path)\n",
    "model.save_pretrained(finetuned_model_path, safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "412c3366-2b42-42ff-acc1-7140c1bc2142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenzier = AutoTokenizer.from_pretrained(finetuned_model_path)\n",
    "model = DistilBertForMaskedLM.from_pretrained(finetuned_model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8a8764c-7e07-429e-9d5c-1c0fc7f1cf69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_topk_for_masked(tokenizer, model, text, topk=5):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "    token_logits = model(**inputs).logits\n",
    "    #print(token_logits.shape)\n",
    "    \n",
    "    # [MASK]의 위치를 찾고, 해당 logits을 추출합니다.\n",
    "    #print(torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id))\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    #print(mask_token_index)\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    #print(mask_token_logits)\n",
    "    \n",
    "    # 가장 큰 logits값을 가지는 [MASK] 후보를 선택합니다.\n",
    "    top_5_tokens = torch.topk(mask_token_logits, topk, dim=1).indices[0].tolist()\n",
    "    \n",
    "    return top_5_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d00e67e-ac20-4add-b497-9e75df421a03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'input text: 미세먼지가 심하면 차량 2부제와 [MASK] 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 ##의 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 ##가 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 ##이 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 ##와 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 ##지 비상저감조치를 시행'\n",
      "'input text: 미[MASK]먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##가먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##일먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##나먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##와먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미##리먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'input text: 미세먼지가 심[MASK] 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##미 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##의 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##한 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##지 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 미세먼지가 심##장 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'input text: [MASK]가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 그가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 일가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 목가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 한가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'>>> 이가 심하면 차량 2부제와 같은 비상저감조치를 시행'\n",
      "'input text: 미세먼지가 심하면 차량 2부제와 같은 [MASK]를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 같은 ##와를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 같은 도를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 같은 나를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 같은 ##가를 시행'\n",
      "'>>> 미세먼지가 심하면 차량 2부제와 같은 해를 시행'\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"미세먼지가 심하면 차량 2부제와 [MASK] 비상저감조치를 시행\", \n",
    "    \"미[MASK]먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행\",\n",
    "    \"미세먼지가 심[MASK] 차량 2부제와 같은 비상저감조치를 시행\",\n",
    "    \"[MASK]가 심하면 차량 2부제와 같은 비상저감조치를 시행\",\n",
    "    \"미세먼지가 심하면 차량 2부제와 같은 [MASK]를 시행\"\n",
    "]\n",
    "for text in test_texts:\n",
    "    print(f\"'input text: {text}'\")\n",
    "    topk_tokens = find_topk_for_masked(tokenizer, model, text, topk=5)\n",
    "    for token in topk_tokens:\n",
    "        print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d12515-19dc-44fb-9d01-67153564e8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-4.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-4:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
