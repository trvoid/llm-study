{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/RDYU0FWos2+q9HSRUacH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trvoid/llm-study/blob/main/distilbert/distilbert_pretraining_for_won.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBERT 사전 학습"
      ],
      "metadata": {
        "id": "TndQSYpenwe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토크나이저 훈련"
      ],
      "metadata": {
        "id": "_8PcT5WQwJnv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMagHwqAnjNq",
        "outputId": "1d488509-b199-4f18-e69b-b1efb8073c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tokenizer: won/tokenizers/bpe_5000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/unigram_5000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/wordpiece_5000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/bpe_10000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/unigram_10000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/wordpiece_10000/tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, Unigram, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "corpus_file = \"won/won04-gyojeon.txt\"\n",
        "data_files = [\n",
        "    \"won/won04-gyojeon.txt\",\n",
        "    \"won/won05-gyosa.txt\"\n",
        "]\n",
        "\n",
        "output_dir = \"won/tokenizers\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "vocab_sizes = [5000, 10000]\n",
        "limit_alphabet = 6000\n",
        "min_frequency = 5\n",
        "model_names = [\"bpe\", \"unigram\", \"wordpiece\"]\n",
        "\n",
        "for vocab_size in vocab_sizes:\n",
        "    for model_name in model_names:\n",
        "        if model_name == \"bpe\":\n",
        "            tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "            trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                                vocab_size=vocab_size,\n",
        "                                limit_alphabet=limit_alphabet,\n",
        "                                min_frequency=min_frequency)\n",
        "        elif model_name == \"unigram\":\n",
        "            tokenizer = Tokenizer(Unigram())\n",
        "            trainer = UnigramTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                                unk_token=\"[UNK]\",\n",
        "                                vocab_size=vocab_size,\n",
        "                                limit_alphabet=limit_alphabet,\n",
        "                                min_frequency=min_frequency)\n",
        "        elif model_name == \"wordpiece\":\n",
        "            tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "            trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                                vocab_size=vocab_size,\n",
        "                                limit_alphabet=limit_alphabet,\n",
        "                                min_frequency=min_frequency)\n",
        "\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "        tokenizer.train(data_files, trainer)\n",
        "\n",
        "        tokenizer_dir = os.path.join(output_dir, f\"{model_name}_{vocab_size}\")\n",
        "        if not os.path.exists(tokenizer_dir):\n",
        "            os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "        tokenizer_file = os.path.join(tokenizer_dir, \"tokenizer.json\")\n",
        "        tokenizer.save(tokenizer_file)\n",
        "        print(f\"Saved tokenizer: {tokenizer_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토크나이저 사용"
      ],
      "metadata": {
        "id": "Kufd0dQbhWBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"물질이 개벽되니 정신을 개벽하자\",\n",
        "    \"19.대종사 말씀하시기를 [스승이 법을 새로 내는 일이나\"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    print(\"#\" * 80)\n",
        "    print(\"TEXT: \" + text)\n",
        "    print(\"-\" * 80)\n",
        "    for vocab_size in vocab_sizes:\n",
        "        for model_name in model_names:\n",
        "            try:\n",
        "                tokenizer_file = os.path.join(output_dir, f\"{model_name}_{vocab_size}\", \"tokenizer.json\")\n",
        "                tokenizer = Tokenizer.from_file(tokenizer_file)\n",
        "                print(f\"{model_name:9} {vocab_size:5}: {tokenizer.encode(text).tokens}\")\n",
        "            except Exception as e:\n",
        "                print(f\"{model_name:9} {vocab_size:5}: FAIL\")\n",
        "                print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9iMtgt6rf6E",
        "outputId": "f3552f5e-c841-43c5-943f-a19a3f738825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "TEXT: 물질이 개벽되니 정신을 개벽하자\n",
            "--------------------------------------------------------------------------------\n",
            "bpe        5000: ['물질', '이', '개벽', '되니', '정신을', '개벽', '하자']\n",
            "unigram    5000: ['물질', '이', '개벽', '되니', '정신', '을', '개벽', '하자']\n",
            "wordpiece  5000: ['물질', '##이', '개', '##벽', '##되', '##니', '정신을', '개', '##벽', '##하', '##자']\n",
            "bpe       10000: ['물질이', '개벽', '되니', '정신을', '개벽', '하자']\n",
            "unigram   10000: ['물질이', '개벽', '되니', '정신', '을', '개벽', '하자']\n",
            "wordpiece 10000: ['물질이', '개벽', '##되니', '정신을', '개벽', '##하자']\n",
            "################################################################################\n",
            "TEXT: 19.대종사 말씀하시기를 [스승이 법을 새로 내는 일이나\n",
            "--------------------------------------------------------------------------------\n",
            "bpe        5000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내는', '일이나']\n",
            "unigram    5000: ['19', '.', '대종사', '말씀하시', '기', '를', '[', '스승이', '법', '을', '새로', '내', '는', '일이', '나']\n",
            "wordpiece  5000: ['19', '.', '대종사', '말씀하시기를', '[', '스승', '##이', '법을', '새로', '내', '##는', '일이', '##나']\n",
            "bpe       10000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내는', '일이나']\n",
            "unigram   10000: ['19', '.', '대종사', '말씀하시', '기', '를', '[', '스승이', '법', '을', '새로', '내', '는', '일이', '나']\n",
            "wordpiece 10000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내는', '일이나']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 준비"
      ],
      "metadata": {
        "id": "oibf1vyDwhJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    PreTrainedTokenizerFast,\n",
        "    LineByLineTextDataset,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "tokenizer_dir = os.path.join(output_dir, \"unigram_5000\")\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    tokenizer_dir,\n",
        "    unk_token=\"[UNK]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    mask_token=\"[MASK]\"\n",
        ")\n",
        "print(tokenizer.vocab_size)\n",
        "#tokenizer = Tokenizer.from_file(\"./tokenizer_5000_wordpiece/tokenizer.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "770Bt6IXwkJ7",
        "outputId": "146b4f69-e4bf-4cf2-e7a0-a62a830cbe58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLM을 위한 데이터셋\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=corpus_file,\n",
        "    block_size=128  # 토큰 기준 최대 길이\n",
        ")\n",
        "\n",
        "# MLM 데이터 콜레이터\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFgNzNvBvK6f",
        "outputId": "02f89db3-dbe3-4405-ea39-4882a29519b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 구성"
      ],
      "metadata": {
        "id": "OjTJRzhW01c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertConfig, DistilBertForMaskedLM\n",
        "\n",
        "#dim = 768                 # Hidden size\n",
        "#hidden_dim = 3072         # Intermediate size (dim * 4)\n",
        "#n_layers = 6              # Number of layers\n",
        "#n_heads = 12              # Number of attention heads\n",
        "#max_position_embeddings = 512\n",
        "\n",
        "dim = 60\n",
        "hidden_dim = 128\n",
        "n_layers = 3\n",
        "n_heads = 6\n",
        "max_position_embeddings = 256\n",
        "\n",
        "# 표준 DistilBERT-base 구성과 유사하게 설정\n",
        "config = DistilBertConfig(\n",
        "    vocab_size=tokenizer.vocab_size, # 로드한 토크나이저의 어휘 크기 사용\n",
        "    activation=\"gelu\",\n",
        "    dim=dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    n_layers=n_layers,\n",
        "    n_heads=n_heads,\n",
        "    max_position_embeddings=max_position_embeddings\n",
        "    # dropout, attention_dropout 등 다른 파라미터도 설정 가능\n",
        ")\n",
        "\n",
        "# 모델 초기화 (랜덤 가중치)\n",
        "model = DistilBertForMaskedLM(config=config)\n",
        "print(model.num_parameters())"
      ],
      "metadata": {
        "id": "r7SFwhrA00A6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201c2130-82a1-4428-8c49-a71b3b67a91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "415544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 훈련"
      ],
      "metadata": {
        "id": "Y2zo_B7CB-Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# 학습 인자 설정\n",
        "output_dir = \"./my_distilbert_pretrained_mlm\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=40,       # 실제로는 훨씬 더 많이 필요\n",
        "    per_device_train_batch_size=8, # GPU 메모리에 맞춰 조절\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=True, # 가능하면 True\n",
        "    # learning_rate, weight_decay 등 추가 설정 필요\n",
        ")\n",
        "\n",
        "# Trainer 초기화\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# --- 6. 모델 학습 ---\n",
        "print(\"DistilBERT 구조 모델 사전 학습(MLM)을 시작합니다. (매우 간소화된 예시)\")\n",
        "trainer.train()\n",
        "print(\"사전 학습 완료.\")\n",
        "\n",
        "# --- 7. 모델 저장 ---\n",
        "print(f\"학습된 모델과 토크나이저를 {output_dir}에 저장합니다.\")\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir) # 사용한 토크나이저 함께 저장\n",
        "\n",
        "print(\"저장 완료.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "E1-a25IP1SsQ",
        "outputId": "6f1aed48-3a06-429b-a46c-674733b5429f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT 구조 모델 사전 학습(MLM)을 시작합니다. (매우 간소화된 예시)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5440' max='5440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5440/5440 01:56, Epoch 40/40]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>8.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>7.142500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>6.718400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>6.549400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>6.468000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>6.455900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>6.437200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>6.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>6.422800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>6.415400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사전 학습 완료.\n",
            "학습된 모델과 토크나이저를 ./my_distilbert_pretrained_mlm에 저장합니다.\n",
            "저장 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 사용"
      ],
      "metadata": {
        "id": "8jBX8n5uEpdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용 예시 (저장된 모델 로드)\n",
        "from transformers import AutoTokenizer, DistilBertForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model = DistilBertForMaskedLM.from_pretrained(output_dir)\n",
        "\n",
        "print(len(tokenizer))"
      ],
      "metadata": {
        "id": "C5yHy_90CKWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44345813-152f-40a0-d93f-69538a53ae95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldNrN-DwHkyK",
        "outputId": "56db1d7f-0de0-4e02-b7c1-ccf6bab8a0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "\n",
        "#device = \"cpu\"\n",
        "\n",
        "def find_topk_for_masked(tokenizer, model, text, topk=5):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    #inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n",
        "    if 'token_type_ids' in inputs:\n",
        "        inputs.pop('token_type_ids')\n",
        "\n",
        "    token_logits = model(**inputs).logits\n",
        "    print(token_logits.shape)\n",
        "\n",
        "    # [MASK]의 위치를 찾고, 해당 logits을 추출합니다.\n",
        "    #print(torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id))\n",
        "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "    #print(mask_token_index)\n",
        "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "    #print(mask_token_logits)\n",
        "\n",
        "    # 가장 큰 logits값을 가지는 [MASK] 후보를 선택합니다.\n",
        "    top_5_tokens = torch.topk(mask_token_logits, topk, dim=1).indices[0].tolist()\n",
        "\n",
        "    return top_5_tokens"
      ],
      "metadata": {
        "id": "Cu5BQ8zhGEsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"물질이 개벽되니 [MASK]을 개벽하자\",\n",
        "    \"19.대종사 말씀하시기를 [스승이 [MASK]을 새로 내는 일이나\",\n",
        "    \"19.대종사 [MASK]기를\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    print(f\"'input text: {text}'\")\n",
        "    try:\n",
        "        topk_tokens = find_topk_for_masked(tokenizer, model, text, topk=5)\n",
        "        for token in topk_tokens:\n",
        "            print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Exception: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yERZJRImE_eS",
        "outputId": "6bf61d3e-b473-469a-e670-d130d1091d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'input text: 물질이 개벽되니 [MASK]을 개벽하자'\n",
            "torch.Size([1, 8, 5000])\n",
            "'>>> 물질이 개벽되니 을을 개벽하자'\n",
            "'>>> 물질이 개벽되니 .을 개벽하자'\n",
            "'>>> 물질이 개벽되니 이을 개벽하자'\n",
            "'>>> 물질이 개벽되니 ,을 개벽하자'\n",
            "'>>> 물질이 개벽되니 를을 개벽하자'\n",
            "'input text: 19.대종사 말씀하시기를 [스승이 [MASK]을 새로 내는 일이나'\n",
            "torch.Size([1, 15, 5000])\n",
            "'>>> 19.대종사 말씀하시기를 [스승이 을을 새로 내는 일이나'\n",
            "'>>> 19.대종사 말씀하시기를 [스승이 ,을 새로 내는 일이나'\n",
            "'>>> 19.대종사 말씀하시기를 [스승이 를을 새로 내는 일이나'\n",
            "'>>> 19.대종사 말씀하시기를 [스승이 이을 새로 내는 일이나'\n",
            "'>>> 19.대종사 말씀하시기를 [스승이 .을 새로 내는 일이나'\n",
            "'input text: 19.대종사 [MASK]기를'\n",
            "torch.Size([1, 6, 5000])\n",
            "'>>> 19.대종사 을기를'\n",
            "'>>> 19.대종사 ,기를'\n",
            "'>>> 19.대종사 이기를'\n",
            "'>>> 19.대종사 .기를'\n",
            "'>>> 19.대종사 를기를'\n"
          ]
        }
      ]
    }
  ]
}