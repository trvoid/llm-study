{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/RDYU0FWos2+q9HSRUacH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trvoid/llm-study/blob/main/distilbert/distilbert_pretraining_for_won.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBERT ì‚¬ì „ í•™ìŠµ"
      ],
      "metadata": {
        "id": "TndQSYpenwe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## í† í¬ë‚˜ì´ì € í›ˆë ¨"
      ],
      "metadata": {
        "id": "_8PcT5WQwJnv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMagHwqAnjNq",
        "outputId": "1d488509-b199-4f18-e69b-b1efb8073c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tokenizer: won/tokenizers/bpe_5000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/unigram_5000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/wordpiece_5000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/bpe_10000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/unigram_10000/tokenizer.json\n",
            "Saved tokenizer: won/tokenizers/wordpiece_10000/tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, Unigram, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "corpus_file = \"won/won04-gyojeon.txt\"\n",
        "data_files = [\n",
        "    \"won/won04-gyojeon.txt\",\n",
        "    \"won/won05-gyosa.txt\"\n",
        "]\n",
        "\n",
        "output_dir = \"won/tokenizers\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "vocab_sizes = [5000, 10000]\n",
        "limit_alphabet = 6000\n",
        "min_frequency = 5\n",
        "model_names = [\"bpe\", \"unigram\", \"wordpiece\"]\n",
        "\n",
        "for vocab_size in vocab_sizes:\n",
        "    for model_name in model_names:\n",
        "        if model_name == \"bpe\":\n",
        "            tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "            trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                                vocab_size=vocab_size,\n",
        "                                limit_alphabet=limit_alphabet,\n",
        "                                min_frequency=min_frequency)\n",
        "        elif model_name == \"unigram\":\n",
        "            tokenizer = Tokenizer(Unigram())\n",
        "            trainer = UnigramTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                                unk_token=\"[UNK]\",\n",
        "                                vocab_size=vocab_size,\n",
        "                                limit_alphabet=limit_alphabet,\n",
        "                                min_frequency=min_frequency)\n",
        "        elif model_name == \"wordpiece\":\n",
        "            tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "            trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                                vocab_size=vocab_size,\n",
        "                                limit_alphabet=limit_alphabet,\n",
        "                                min_frequency=min_frequency)\n",
        "\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "        tokenizer.train(data_files, trainer)\n",
        "\n",
        "        tokenizer_dir = os.path.join(output_dir, f\"{model_name}_{vocab_size}\")\n",
        "        if not os.path.exists(tokenizer_dir):\n",
        "            os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "        tokenizer_file = os.path.join(tokenizer_dir, \"tokenizer.json\")\n",
        "        tokenizer.save(tokenizer_file)\n",
        "        print(f\"Saved tokenizer: {tokenizer_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## í† í¬ë‚˜ì´ì € ì‚¬ìš©"
      ],
      "metadata": {
        "id": "Kufd0dQbhWBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"ë¬¼ì§ˆì´ ê°œë²½ë˜ë‹ˆ ì •ì‹ ì„ ê°œë²½í•˜ì\",\n",
        "    \"19.ëŒ€ì¢…ì‚¬ ë§ì”€í•˜ì‹œê¸°ë¥¼ [ìŠ¤ìŠ¹ì´ ë²•ì„ ìƒˆë¡œ ë‚´ëŠ” ì¼ì´ë‚˜\"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    print(\"#\" * 80)\n",
        "    print(\"TEXT: \" + text)\n",
        "    print(\"-\" * 80)\n",
        "    for vocab_size in vocab_sizes:\n",
        "        for model_name in model_names:\n",
        "            try:\n",
        "                tokenizer_file = os.path.join(output_dir, f\"{model_name}_{vocab_size}\", \"tokenizer.json\")\n",
        "                tokenizer = Tokenizer.from_file(tokenizer_file)\n",
        "                print(f\"{model_name:9} {vocab_size:5}: {tokenizer.encode(text).tokens}\")\n",
        "            except Exception as e:\n",
        "                print(f\"{model_name:9} {vocab_size:5}: FAIL\")\n",
        "                print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9iMtgt6rf6E",
        "outputId": "f3552f5e-c841-43c5-943f-a19a3f738825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "TEXT: ë¬¼ì§ˆì´ ê°œë²½ë˜ë‹ˆ ì •ì‹ ì„ ê°œë²½í•˜ì\n",
            "--------------------------------------------------------------------------------\n",
            "bpe        5000: ['ë¬¼ì§ˆ', 'ì´', 'ê°œë²½', 'ë˜ë‹ˆ', 'ì •ì‹ ì„', 'ê°œë²½', 'í•˜ì']\n",
            "unigram    5000: ['ë¬¼ì§ˆ', 'ì´', 'ê°œë²½', 'ë˜ë‹ˆ', 'ì •ì‹ ', 'ì„', 'ê°œë²½', 'í•˜ì']\n",
            "wordpiece  5000: ['ë¬¼ì§ˆ', '##ì´', 'ê°œ', '##ë²½', '##ë˜', '##ë‹ˆ', 'ì •ì‹ ì„', 'ê°œ', '##ë²½', '##í•˜', '##ì']\n",
            "bpe       10000: ['ë¬¼ì§ˆì´', 'ê°œë²½', 'ë˜ë‹ˆ', 'ì •ì‹ ì„', 'ê°œë²½', 'í•˜ì']\n",
            "unigram   10000: ['ë¬¼ì§ˆì´', 'ê°œë²½', 'ë˜ë‹ˆ', 'ì •ì‹ ', 'ì„', 'ê°œë²½', 'í•˜ì']\n",
            "wordpiece 10000: ['ë¬¼ì§ˆì´', 'ê°œë²½', '##ë˜ë‹ˆ', 'ì •ì‹ ì„', 'ê°œë²½', '##í•˜ì']\n",
            "################################################################################\n",
            "TEXT: 19.ëŒ€ì¢…ì‚¬ ë§ì”€í•˜ì‹œê¸°ë¥¼ [ìŠ¤ìŠ¹ì´ ë²•ì„ ìƒˆë¡œ ë‚´ëŠ” ì¼ì´ë‚˜\n",
            "--------------------------------------------------------------------------------\n",
            "bpe        5000: ['19', '.', 'ëŒ€ì¢…ì‚¬', 'ë§ì”€í•˜ì‹œê¸°ë¥¼', '[', 'ìŠ¤ìŠ¹ì´', 'ë²•ì„', 'ìƒˆë¡œ', 'ë‚´ëŠ”', 'ì¼ì´ë‚˜']\n",
            "unigram    5000: ['19', '.', 'ëŒ€ì¢…ì‚¬', 'ë§ì”€í•˜ì‹œ', 'ê¸°', 'ë¥¼', '[', 'ìŠ¤ìŠ¹ì´', 'ë²•', 'ì„', 'ìƒˆë¡œ', 'ë‚´', 'ëŠ”', 'ì¼ì´', 'ë‚˜']\n",
            "wordpiece  5000: ['19', '.', 'ëŒ€ì¢…ì‚¬', 'ë§ì”€í•˜ì‹œê¸°ë¥¼', '[', 'ìŠ¤ìŠ¹', '##ì´', 'ë²•ì„', 'ìƒˆë¡œ', 'ë‚´', '##ëŠ”', 'ì¼ì´', '##ë‚˜']\n",
            "bpe       10000: ['19', '.', 'ëŒ€ì¢…ì‚¬', 'ë§ì”€í•˜ì‹œê¸°ë¥¼', '[', 'ìŠ¤ìŠ¹ì´', 'ë²•ì„', 'ìƒˆë¡œ', 'ë‚´ëŠ”', 'ì¼ì´ë‚˜']\n",
            "unigram   10000: ['19', '.', 'ëŒ€ì¢…ì‚¬', 'ë§ì”€í•˜ì‹œ', 'ê¸°', 'ë¥¼', '[', 'ìŠ¤ìŠ¹ì´', 'ë²•', 'ì„', 'ìƒˆë¡œ', 'ë‚´', 'ëŠ”', 'ì¼ì´', 'ë‚˜']\n",
            "wordpiece 10000: ['19', '.', 'ëŒ€ì¢…ì‚¬', 'ë§ì”€í•˜ì‹œê¸°ë¥¼', '[', 'ìŠ¤ìŠ¹ì´', 'ë²•ì„', 'ìƒˆë¡œ', 'ë‚´ëŠ”', 'ì¼ì´ë‚˜']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë°ì´í„°ì…‹ ì¤€ë¹„"
      ],
      "metadata": {
        "id": "oibf1vyDwhJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    PreTrainedTokenizerFast,\n",
        "    LineByLineTextDataset,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "tokenizer_dir = os.path.join(output_dir, \"unigram_5000\")\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    tokenizer_dir,\n",
        "    unk_token=\"[UNK]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    mask_token=\"[MASK]\"\n",
        ")\n",
        "print(tokenizer.vocab_size)\n",
        "#tokenizer = Tokenizer.from_file(\"./tokenizer_5000_wordpiece/tokenizer.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "770Bt6IXwkJ7",
        "outputId": "146b4f69-e4bf-4cf2-e7a0-a62a830cbe58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLMì„ ìœ„í•œ ë°ì´í„°ì…‹\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=corpus_file,\n",
        "    block_size=128  # í† í° ê¸°ì¤€ ìµœëŒ€ ê¸¸ì´\n",
        ")\n",
        "\n",
        "# MLM ë°ì´í„° ì½œë ˆì´í„°\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFgNzNvBvK6f",
        "outputId": "02f89db3-dbe3-4405-ea39-4882a29519b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ëª¨ë¸ êµ¬ì„±"
      ],
      "metadata": {
        "id": "OjTJRzhW01c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertConfig, DistilBertForMaskedLM\n",
        "\n",
        "#dim = 768                 # Hidden size\n",
        "#hidden_dim = 3072         # Intermediate size (dim * 4)\n",
        "#n_layers = 6              # Number of layers\n",
        "#n_heads = 12              # Number of attention heads\n",
        "#max_position_embeddings = 512\n",
        "\n",
        "dim = 60\n",
        "hidden_dim = 128\n",
        "n_layers = 3\n",
        "n_heads = 6\n",
        "max_position_embeddings = 256\n",
        "\n",
        "# í‘œì¤€ DistilBERT-base êµ¬ì„±ê³¼ ìœ ì‚¬í•˜ê²Œ ì„¤ì •\n",
        "config = DistilBertConfig(\n",
        "    vocab_size=tokenizer.vocab_size, # ë¡œë“œí•œ í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ í¬ê¸° ì‚¬ìš©\n",
        "    activation=\"gelu\",\n",
        "    dim=dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    n_layers=n_layers,\n",
        "    n_heads=n_heads,\n",
        "    max_position_embeddings=max_position_embeddings\n",
        "    # dropout, attention_dropout ë“± ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë„ ì„¤ì • ê°€ëŠ¥\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™” (ëœë¤ ê°€ì¤‘ì¹˜)\n",
        "model = DistilBertForMaskedLM(config=config)\n",
        "print(model.num_parameters())"
      ],
      "metadata": {
        "id": "r7SFwhrA00A6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201c2130-82a1-4428-8c49-a71b3b67a91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "415544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ëª¨ë¸ í›ˆë ¨"
      ],
      "metadata": {
        "id": "Y2zo_B7CB-Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# í•™ìŠµ ì¸ì ì„¤ì •\n",
        "output_dir = \"./my_distilbert_pretrained_mlm\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=40,       # ì‹¤ì œë¡œëŠ” í›¨ì”¬ ë” ë§ì´ í•„ìš”\n",
        "    per_device_train_batch_size=8, # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì ˆ\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=True, # ê°€ëŠ¥í•˜ë©´ True\n",
        "    # learning_rate, weight_decay ë“± ì¶”ê°€ ì„¤ì • í•„ìš”\n",
        ")\n",
        "\n",
        "# Trainer ì´ˆê¸°í™”\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# --- 6. ëª¨ë¸ í•™ìŠµ ---\n",
        "print(\"DistilBERT êµ¬ì¡° ëª¨ë¸ ì‚¬ì „ í•™ìŠµ(MLM)ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ë§¤ìš° ê°„ì†Œí™”ëœ ì˜ˆì‹œ)\")\n",
        "trainer.train()\n",
        "print(\"ì‚¬ì „ í•™ìŠµ ì™„ë£Œ.\")\n",
        "\n",
        "# --- 7. ëª¨ë¸ ì €ì¥ ---\n",
        "print(f\"í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ {output_dir}ì— ì €ì¥í•©ë‹ˆë‹¤.\")\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir) # ì‚¬ìš©í•œ í† í¬ë‚˜ì´ì € í•¨ê»˜ ì €ì¥\n",
        "\n",
        "print(\"ì €ì¥ ì™„ë£Œ.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "E1-a25IP1SsQ",
        "outputId": "6f1aed48-3a06-429b-a46c-674733b5429f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT êµ¬ì¡° ëª¨ë¸ ì‚¬ì „ í•™ìŠµ(MLM)ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ë§¤ìš° ê°„ì†Œí™”ëœ ì˜ˆì‹œ)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5440' max='5440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5440/5440 01:56, Epoch 40/40]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>8.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>7.142500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>6.718400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>6.549400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>6.468000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>6.455900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>6.437200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>6.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>6.422800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>6.415400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‚¬ì „ í•™ìŠµ ì™„ë£Œ.\n",
            "í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ./my_distilbert_pretrained_mlmì— ì €ì¥í•©ë‹ˆë‹¤.\n",
            "ì €ì¥ ì™„ë£Œ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ëª¨ë¸ ì‚¬ìš©"
      ],
      "metadata": {
        "id": "8jBX8n5uEpdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‚¬ìš© ì˜ˆì‹œ (ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ)\n",
        "from transformers import AutoTokenizer, DistilBertForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model = DistilBertForMaskedLM.from_pretrained(output_dir)\n",
        "\n",
        "print(len(tokenizer))"
      ],
      "metadata": {
        "id": "C5yHy_90CKWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44345813-152f-40a0-d93f-69538a53ae95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldNrN-DwHkyK",
        "outputId": "56db1d7f-0de0-4e02-b7c1-ccf6bab8a0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "\n",
        "#device = \"cpu\"\n",
        "\n",
        "def find_topk_for_masked(tokenizer, model, text, topk=5):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    #inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n",
        "    if 'token_type_ids' in inputs:\n",
        "        inputs.pop('token_type_ids')\n",
        "\n",
        "    token_logits = model(**inputs).logits\n",
        "    print(token_logits.shape)\n",
        "\n",
        "    # [MASK]ì˜ ìœ„ì¹˜ë¥¼ ì°¾ê³ , í•´ë‹¹ logitsì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "    #print(torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id))\n",
        "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "    #print(mask_token_index)\n",
        "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "    #print(mask_token_logits)\n",
        "\n",
        "    # ê°€ì¥ í° logitsê°’ì„ ê°€ì§€ëŠ” [MASK] í›„ë³´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
        "    top_5_tokens = torch.topk(mask_token_logits, topk, dim=1).indices[0].tolist()\n",
        "\n",
        "    return top_5_tokens"
      ],
      "metadata": {
        "id": "Cu5BQ8zhGEsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"ë¬¼ì§ˆì´ ê°œë²½ë˜ë‹ˆ [MASK]ì„ ê°œë²½í•˜ì\",\n",
        "    \"19.ëŒ€ì¢…ì‚¬ ë§ì”€í•˜ì‹œê¸°ë¥¼ [ìŠ¤ìŠ¹ì´ [MASK]ì„ ìƒˆë¡œ ë‚´ëŠ” ì¼ì´ë‚˜\",\n",
        "    \"19.ëŒ€ì¢…ì‚¬ [MASK]ê¸°ë¥¼\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    print(f\"'input text: {text}'\")\n",
        "    try:\n",
        "        topk_tokens = find_topk_for_masked(tokenizer, model, text, topk=5)\n",
        "        for token in topk_tokens:\n",
        "            print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Exception: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yERZJRImE_eS",
        "outputId": "6bf61d3e-b473-469a-e670-d130d1091d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'input text: ë¬¼ì§ˆì´ ê°œë²½ë˜ë‹ˆ [MASK]ì„ ê°œë²½í•˜ì'\n",
            "torch.Size([1, 8, 5000])\n",
            "'>>> ë¬¼ì§ˆì´ ê°œë²½ë˜ë‹ˆ ì„ì„ ê°œë²½í•˜ì'\n",
            "'>>> ë¬¼ì§ˆì´ ê°œë²½ë˜ë‹ˆ .ì„ ê°œë²½í•˜ì'\n",
            "'>>> ë¬¼ì§ˆì´ ê°œë²½ë˜ë‹ˆ ì´ì„ ê°œë²½í•˜ì'\n",
            "'>>> ë¬¼ì§ˆì´ ê°œë²½ë˜ë‹ˆ ,ì„ ê°œë²½í•˜ì'\n",
            "'>>> ë¬¼ì§ˆì´ ê°œë²½ë˜ë‹ˆ ë¥¼ì„ ê°œë²½í•˜ì'\n",
            "'input text: 19.ëŒ€ì¢…ì‚¬ ë§ì”€í•˜ì‹œê¸°ë¥¼ [ìŠ¤ìŠ¹ì´ [MASK]ì„ ìƒˆë¡œ ë‚´ëŠ” ì¼ì´ë‚˜'\n",
            "torch.Size([1, 15, 5000])\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ ë§ì”€í•˜ì‹œê¸°ë¥¼ [ìŠ¤ìŠ¹ì´ ì„ì„ ìƒˆë¡œ ë‚´ëŠ” ì¼ì´ë‚˜'\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ ë§ì”€í•˜ì‹œê¸°ë¥¼ [ìŠ¤ìŠ¹ì´ ,ì„ ìƒˆë¡œ ë‚´ëŠ” ì¼ì´ë‚˜'\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ ë§ì”€í•˜ì‹œê¸°ë¥¼ [ìŠ¤ìŠ¹ì´ ë¥¼ì„ ìƒˆë¡œ ë‚´ëŠ” ì¼ì´ë‚˜'\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ ë§ì”€í•˜ì‹œê¸°ë¥¼ [ìŠ¤ìŠ¹ì´ ì´ì„ ìƒˆë¡œ ë‚´ëŠ” ì¼ì´ë‚˜'\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ ë§ì”€í•˜ì‹œê¸°ë¥¼ [ìŠ¤ìŠ¹ì´ .ì„ ìƒˆë¡œ ë‚´ëŠ” ì¼ì´ë‚˜'\n",
            "'input text: 19.ëŒ€ì¢…ì‚¬ [MASK]ê¸°ë¥¼'\n",
            "torch.Size([1, 6, 5000])\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ ì„ê¸°ë¥¼'\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ ,ê¸°ë¥¼'\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ ì´ê¸°ë¥¼'\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ .ê¸°ë¥¼'\n",
            "'>>> 19.ëŒ€ì¢…ì‚¬ ë¥¼ê¸°ë¥¼'\n"
          ]
        }
      ]
    }
  ]
}