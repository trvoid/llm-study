{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/trvoid/llm-study/blob/main/bert/getting_started_with_distilbert_for_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cvu_6JOiK0j"
   },
   "source": [
    "# DistilBERT for QA 시작하기\n",
    "\n",
    "이 실습은 아래 문서의 내용을 토대로 진행하였습니다.\n",
    "\n",
    "* [Fine-Tuning DistilBERT for Question Answering](https://machinelearningmastery.com/fine-tuning-distilbert-for-question-answering/), By Muhammad Asad Iqbal Khan on March 29, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4PMWlGHibc1"
   },
   "source": [
    "## 1. 해결하고자 하는 문제\n",
    "\n",
    "다음과 같은 유형의 문제를 언어 모델을 사용하여 풀고자 합니다.\n",
    "\n",
    "* 지문(context): Tom sits on a bench.\n",
    "* 질문(question): Where does Tom sit?\n",
    "* 답변(answer): bench (지문에서 첫번째 문자의 위치를 0이라고 할 때 답변의 시작 위치는 14, 끝 위치는 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WekkNEWOh0OC",
    "outputId": "3952d28b-0f0c-4b2d-da72-dd0aae3d5782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_char=14, end_char=19\n"
     ]
    }
   ],
   "source": [
    "context = \"Tom sits on a bench.\"\n",
    "answer = \"bench\"\n",
    "\n",
    "start_char = context.find(answer)\n",
    "end_char = start_char + len(answer)\n",
    "print(f\"start_char={start_char}, end_char={end_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YerIbsCNm9wH"
   },
   "source": [
    "주어진 지문으로부터 질문에 답변하는 방식에 따라 아래 두 가지로 구분할 수 있습니다.\n",
    "\n",
    "* 추출형 질의응답 (Extractive Question Answering): 주어진 지문 내에서 질문에 대한 답변에 해당하는 부분을 그대로 찾아 추출하는 방식\n",
    "* 생성형 질의응답 (Abstractive Question Answering): 주어진 지문의 내용을 이해하고 요약하거나 재구성하여 질문에 대한 답변을 새롭게 생성하는 방식\n",
    "\n",
    "여기서 다루는 문제는 추출형 질의응답에 해당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLRK87J4oijH"
   },
   "source": [
    "## 2. 문제 해결 방안\n",
    "\n",
    "언어 모델 훈련에 사용할 데이터를 아래의 형식에 맞추어 준비합니다.\n",
    "\n",
    "* 입력 데이터: 질문과 지문을 토큰화하고 이어붙인 배열\n",
    "  * \"[CLS] *question* [SEP] *context* [SEP]\"\n",
    "* 정답 데이터: 입력 데이터에서 답변의 시작 토큰과 끝 토큰의 위치 정보\n",
    "  * 입력 데이터에서 답변의 시작 토큰 위치\n",
    "  * 입력 데이터에서 답변의 끝 토큰 위치\n",
    "\n",
    "위 형식의 데이터를 대상으로 훈련하는 과정은 다음과 같습니다.\n",
    "\n",
    "* 모델과 데이터셋 선택\n",
    "  * 언어 이해를 목적으로 훈련된 기본 모델 선택\n",
    "  * 질의응답 작업을 위해 미세조정 훈련을 수행할 모델 선택\n",
    "  * 질의응답 미세조정 훈련에 사용할 데이터셋 선택\n",
    "* 미세조정 훈련\n",
    "  1. 미세조정 모델로 정답을 예측하고 손실 계산\n",
    "  2. 미세조정 모델 파라미터 갱신\n",
    "\n",
    "이 실습에서 사용할 모델과 데이터셋은 다음과 같습니다.\n",
    "\n",
    "1. [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert):기본 모델 (단어에 대한 단순 임베딩이 아니라 맥락을 고려한 임베딩 수행)\n",
    "2. [DistilBertForQuestionAnswering](https://huggingface.co/docs/transformers/en/model_doc/distilbert?usage=Pipeline#transformers.DistilBertForQuestionAnswering): DistilBERT 모델에 질의응답 층을 추가한 것으로서 질의응답 미세조정 훈련을 위한 모델\n",
    "3. [SQuAD](https://huggingface.co/datasets/rajpurkar/squad): 질의응답 미세조정 훈련을 위한 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wjt9JTaBkod"
   },
   "source": [
    "## 3. 데이터셋 적재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDwsB6s4lTMM"
   },
   "source": [
    "`datasets` 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oB02kkoqvW08",
    "outputId": "30176221-d22f-403c-9ae8-677cd8996e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (0.26.0)\n",
      "Requirement already satisfied: packaging in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (1.15.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPqrfHggldKK"
   },
   "source": [
    "`squad` 데이터셋을 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jT4hoDawu1Sx"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtE0YtRFk6md"
   },
   "source": [
    "`dataset`의 구조를 출력해서 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bl-pxd78vrij",
    "outputId": "e20632a3-d9a9-437c-9320-810dfde54b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSb5USfCkbPl"
   },
   "source": [
    "\"train\" 항목의 첫번째 데이터를 출력해서 실제로 어떤 값을 가지고 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yza3i4GpIFzP",
    "outputId": "0e55d4f4-b409-44cc-e38d-2d45d75d1684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"5733be284776f41900661182\",\n",
      "    \"title\": \"University_of_Notre_Dame\",\n",
      "    \"context\": \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\",\n",
      "    \"question\": \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\",\n",
      "    \"answers\": {\n",
      "        \"text\": [\n",
      "            \"Saint Bernadette Soubirous\"\n",
      "        ],\n",
      "        \"answer_start\": [\n",
      "            515\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(dataset[\"train\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuvzgR2fC3-C"
   },
   "source": [
    "## 4. 토크나이저와 모델 적재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5mFZO1ZmAnr"
   },
   "source": [
    "`transformers` 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "poX7Xe7pEOso",
    "outputId": "cc737986-a986-4e65-a0fa-68b92b03753f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo6UslWcmIpO"
   },
   "source": [
    "기본 모델 `distilbert-base-uncased`를 지정하고 `DistilBertTokenizerFast` 클래스를 사용하여 토크나이저를, `DistilBertForQuestionAnswering` 클래스를 사용하여 모델을 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zADW5Gsu-Gb",
    "outputId": "c9740f3f-c8cf-43cc-aae3-587b9d282916"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 10:24:17.734258: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-07 10:24:17.742564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743989057.751006 2399981 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743989057.753582 2399981 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743989057.761568 2399981 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743989057.761576 2399981 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743989057.761578 2399981 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743989057.761578 2399981 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-07 10:24:17.764097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "\n",
    "# Load tokenizer and model\n",
    "base_model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(base_model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2H3lSaUFL81"
   },
   "source": [
    "## 5. 토크나이저 테스트\n",
    "\n",
    "훈련 데이터 준비 과정에 대한 이해를 돕기 위하여 간단한 예제로 토크나이저를 테스트해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O4PgV3IaEoVt"
   },
   "outputs": [],
   "source": [
    "questions = [\"Where does Tom sits?\"]\n",
    "contexts = [\"Tom sits on a bench.\"]\n",
    "\n",
    "inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=15,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbXDTqCZpKrg"
   },
   "source": [
    "토크나이저 실행 결과인 `inputs`의 유형과 키 항목들을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrIII6NIpRwa",
    "outputId": "06fffcfa-8cc2-4220-f607-437214d7a379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'attention_mask', 'offset_mapping'])\n"
     ]
    }
   ],
   "source": [
    "print(type(inputs))\n",
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HioUORAqFN-"
   },
   "source": [
    "각각의 키 항목들에 해당하는 값을 출력하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYS25q6opxHd",
    "outputId": "493b2f82-85ef-4441-9ec6-86d2ffa06a78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2073, 2515, 3419, 7719, 1029, 102, 3419, 7719, 2006, 1037, 6847, 1012, 102, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]\n",
      "offset_mapping: [[(0, 0), (0, 5), (6, 10), (11, 14), (15, 19), (19, 20), (0, 0), (0, 3), (4, 8), (9, 11), (12, 13), (14, 19), (19, 20), (0, 0), (0, 0)]]\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids: \", end=\"\")\n",
    "print(inputs[\"input_ids\"])\n",
    "\n",
    "print(\"attention_mask: \", end=\"\")\n",
    "print(inputs[\"attention_mask\"])\n",
    "\n",
    "print(\"offset_mapping: \", end=\"\")\n",
    "print(inputs[\"offset_mapping\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a81S7uWiqPpj"
   },
   "source": [
    "`input_ids`에 해당하는 값을 토큰 문자열로 변환해서 보면 각각의 키 항목들의 의미를 좀 더 쉽게 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5UmQXnyFwhf",
    "outputId": "2beee27a-a4ff-4b36-e4bb-912bd3a9aa19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_strs: ['[CLS]', 'where', 'does', 'tom', 'sits', '?', '[SEP]', 'tom', 'sits', 'on', 'a', 'bench', '.', '[SEP]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "token_strs = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "print(\"token_strs: \", end=\"\")\n",
    "print(token_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZB_TDAzq7fz"
   },
   "source": [
    "`token_strs`을 살펴 보면 답변에 해당하는 토큰 'bench'의 시작 위치와 끝 위치가  각각 11임을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI6-4E_lK9UN"
   },
   "source": [
    "## 6. 토큰 배열에서 답변의 시작 토큰 위치와 끝 토큰 위치 찾는 방법\n",
    "\n",
    "토크나이저 출력 결과 중에서 활용해야 할 정보가 하나 더 있는데 그것은 sequence_ids입니다. 이것으로부터 각각의 토큰이 토크나이저에 입력한 문자열들 중에서 몇 번째 문자열에 해당하는 것인지 파악할 수 있습니다.\n",
    "\n",
    "* sequence_ids의 값이 0이면 토크나이저의 첫번째 인자(`questions`)로부터 온 토큰\n",
    "* sequence_ids의 값이 1이면 토크나이저의 두번째 인자(`contexts`)로부터 온 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GizydGWep4-2",
    "outputId": "8e755e94-b887-4471-c045-739a55f6c4d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(sequence_ids): <class 'method'>\n",
      "sequence_ids(0): [None, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(\"type(sequence_ids): \", end=\"\")\n",
    "print(type(inputs.sequence_ids))\n",
    "\n",
    "print(\"sequence_ids(0): \", end=\"\")\n",
    "print(inputs.sequence_ids(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKCMy2scrHxf"
   },
   "source": [
    "이제 토큰 배열에서 답변의 시작 토큰 위치와 끝 토큰 위치를 찾기 위해 필요로 하는 정보는 모두 확보하였습니다.\n",
    "\n",
    "* `inputs.sequence_ids(0): [None, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, None, None]`\n",
    "* `inputs[\"offset_mapping\"][0]: [(0, 0), (0, 5), (6, 10), (11, 14), (15, 19), (19, 20), (0, 0), (0, 3), (4, 8), (9, 11), (12, 13), (14, 19), (19, 20), (0, 0), (0, 0)]`\n",
    "* `start_char: 14`\n",
    "* `end_char: 19`\n",
    "\n",
    "이 정보들을 활용하여 답변의 위치를 기계적으로 찾는 과정은 다음과 같습니다.\n",
    "\n",
    "1. sequence_ids에서 context에 해당하는 토큰들 위치 찾기\n",
    "  1. 값이 1인 요소들 중에서 첫번째 것의 위치: `7`\n",
    "  2. 값이 1인 요소들 중에서 마지막 것의 위치: `12`\n",
    "2. offset_mapping에서 context에 해당하는 토큰들을 대상으로 답변의 시작 위치와 끝 위치에 해당하는 토큰 위치 찾기\n",
    "  1. `start_char` 값인 14를 포함하는 offset_mapping 튜플의 위치: `11`\n",
    "  2. `end_char` 값인 19를 포함하는 offset_mapping 튜플의 위치: `11`\n",
    "\n",
    "답변의 시작 토큰 위치와 끝 토큰의 위치가 훈련 데이터에서 입력 데이터에 대한 정답으로 간주되는 값입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiXcWOvnTjn8"
   },
   "source": [
    "## 7. 훈련 데이터 생성\n",
    "\n",
    "원본 데이터셋에서 개별 데이터 항목은 아래와 같은 구조를 가집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "G8hKTEhUwnct"
   },
   "outputs": [],
   "source": [
    "examples_like_squad = [\n",
    "    {\n",
    "        \"context\": \"Tom sits on a bench.\",\n",
    "        \"question\": \"Where does Tom sit?\",\n",
    "        \"answers\": {\n",
    "            \"text\": [\"bench\"],\n",
    "            \"answer_start\": [14]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf61oon22l5y"
   },
   "source": [
    "훈련용 데이터 형식으로 변환하는 전처리 함수는 아래와 같은 구조의 데이터를 인자로 받아서 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LdjcAFdP0PpC"
   },
   "outputs": [],
   "source": [
    "examples_for_preprocess = {\n",
    "    \"context\": [\"Tom sits on a bench.\"],\n",
    "    \"question\": [\"Where does Tom sit?\"],\n",
    "    \"answers\": [\n",
    "        {\n",
    "            \"text\": [\"bench\"],\n",
    "            \"answer_start\": [14]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7VZuSSI3hyP"
   },
   "source": [
    "데이터셋을 입력으로 받아서 토큰 배열로 만들고 답변의 시작 토큰 위치와 끝 토큰의 위치를 찾아서 훈련용 데이터를 만드는 전처리 함수의 구현은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2r1EwGLZvFAt"
   },
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def preprocess_function(examples, max_length=384):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offsets[context_start][0] > end_char or offsets[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise find the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offsets[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offsets[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGFucTtw3-Aq"
   },
   "source": [
    "위에서 준비한 테스트용 데이터셋을 사용하여 전처리 함수를 실행해 봅니다. 화면 표시의 편의를 위하여 `max_length`의 값을 15로 지정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHGyiwPaUKZU",
    "outputId": "7bb63427-7e0f-4993-c6ef-f788a9779ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2073, 2515, 3419, 4133, 1029, 102, 3419, 7719, 2006, 1037, 6847, 1012, 102, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]\n",
      "start_positions: [11]\n",
      "end_positions: [11]\n"
     ]
    }
   ],
   "source": [
    "results = preprocess_function(examples_for_preprocess, max_length=15)\n",
    "print(f'input_ids: {results[\"input_ids\"]}')\n",
    "print(f'attention_mask: {results[\"attention_mask\"]}')\n",
    "print(f'start_positions: {results[\"start_positions\"]}')\n",
    "print(f'end_positions: {results[\"end_positions\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d2345c97cf9e4f4a9c1a1344c928fdf0",
      "a4088c8d15fb41efb56ca8295c7f25b5",
      "87df2842719045488595c9725e386297",
      "6de2a2c502be4e4fa4770ea0eb4fe94e",
      "85e055a4dcaf4131ad4cad24b5b2410c",
      "32fa3ff8af104c4d9f23ed652c721e20",
      "30d72eedc6714e21a7f200cb0abcfd33",
      "ed9c813e80fc43b096bda875f665b176",
      "1ade48e3bc6d4a46a86b599e87ea406b",
      "8eabe883b07b4d968dd2cd8bff9ca264",
      "6de4057a54f84d10a3dcc7e0535a86e5"
     ]
    },
    "id": "GqJYhmQOT4Vc",
    "outputId": "7ae35b34-aef5-463d-cb21-5aa9cd97a4bf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe342d953cc54b06aa638710175f9ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply preprocessing to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function,\n",
    "                                 batched=True,\n",
    "                                 remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYA38CqSWOYM"
   },
   "source": [
    "## 8. 미세조정 훈련\n",
    "\n",
    "이제 SQuAD 데이터셋을 대상으로 DistilBertForQuestionAnswering 모델을 훈련합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EZmfn1Afm36",
    "outputId": "77251fbf-6eb2-4883-d3ec-13a4e76b841a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: accelerate in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (0.26.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: requests in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "jlTnf83-T_xf",
    "outputId": "472dbaf4-5e88-4834-cec6-147aa9e3b320"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjeong/DevEnv/py312/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_2399981/1952905881.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16425' max='16425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16425/16425 18:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.192100</td>\n",
       "      <td>1.152386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.963400</td>\n",
       "      <td>1.094137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>1.138754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-distilbert-squad/tokenizer_config.json',\n",
       " './fine-tuned-distilbert-squad/special_tokens_map.json',\n",
       " './fine-tuned-distilbert-squad/vocab.txt',\n",
       " './fine-tuned-distilbert-squad/added_tokens.json',\n",
       " './fine-tuned-distilbert-squad/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model and save the results\n",
    "trainer.train()\n",
    "\n",
    "finetuned_model_path = \"./fine-tuned-distilbert-squad\"\n",
    "model.save_pretrained(finetuned_model_path)\n",
    "tokenizer.save_pretrained(finetuned_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZdlfDhR5Lj-"
   },
   "source": [
    "## 9. 미세조정 훈련 모델로 질의응답 수행\n",
    "\n",
    "지문과 질문을 인자로 받아서 답변을 찾아 반환하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JwbCfrNC5bAL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_answer(tokenizer, model, context, question):\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    print(f\"input_tokens: {input_tokens}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(predict_answer_tokens)\n",
    "\n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmJKDKhkYncw"
   },
   "source": [
    "먼저 질의응답 훈련을 하지 않은 기본 모델로 질의응답을 수행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mTL_JdURYFCU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens: ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'nice', 'puppet', '[SEP]']\n",
      "answer by base model: ['a', 'nice', 'puppet', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "test_context = \"Jim Henson was a nice puppet\"\n",
    "test_question = \"Who was Jim Henson?\"\n",
    "\n",
    "base_tokenizer = DistilBertTokenizerFast.from_pretrained(base_model_name)\n",
    "base_model = DistilBertForQuestionAnswering.from_pretrained(base_model_name)\n",
    "\n",
    "test_output_tokens = predict_answer(base_tokenizer, base_model, test_context, test_question)\n",
    "print(f\"answer by base model: {test_output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74uJn51vYvbU"
   },
   "source": [
    "이제 질의응답 데이터에 대하여 미세조정 훈련을 거친 모델로 답변을 예측하고 기본 모델의 예측 결과와 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jbO3_1mPYbf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens: ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'nice', 'puppet', '[SEP]']\n",
      "answer by finetuned model: ['a', 'nice', 'puppet']\n"
     ]
    }
   ],
   "source": [
    "finetuned_tokenizer = DistilBertTokenizerFast.from_pretrained(finetuned_model_path)\n",
    "finetuned_model = DistilBertForQuestionAnswering.from_pretrained(finetuned_model_path)\n",
    "\n",
    "test_output_tokens = predict_answer(finetuned_tokenizer, finetuned_model, test_context, test_question)\n",
    "print(f\"answer by finetuned model: {test_output_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3814566108.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m답변이 기대한 바와 일치하는지 확인합니다.\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "답변이 기대한 바와 일치하는지 확인합니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZ3y78jzXbpSmLBC0mDN3f",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ade48e3bc6d4a46a86b599e87ea406b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30d72eedc6714e21a7f200cb0abcfd33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32fa3ff8af104c4d9f23ed652c721e20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de2a2c502be4e4fa4770ea0eb4fe94e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8eabe883b07b4d968dd2cd8bff9ca264",
      "placeholder": "​",
      "style": "IPY_MODEL_6de4057a54f84d10a3dcc7e0535a86e5",
      "value": " 10570/10570 [00:15&lt;00:00, 788.93 examples/s]"
     }
    },
    "6de4057a54f84d10a3dcc7e0535a86e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85e055a4dcaf4131ad4cad24b5b2410c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87df2842719045488595c9725e386297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed9c813e80fc43b096bda875f665b176",
      "max": 10570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ade48e3bc6d4a46a86b599e87ea406b",
      "value": 10570
     }
    },
    "8eabe883b07b4d968dd2cd8bff9ca264": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4088c8d15fb41efb56ca8295c7f25b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32fa3ff8af104c4d9f23ed652c721e20",
      "placeholder": "​",
      "style": "IPY_MODEL_30d72eedc6714e21a7f200cb0abcfd33",
      "value": "Map: 100%"
     }
    },
    "d2345c97cf9e4f4a9c1a1344c928fdf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4088c8d15fb41efb56ca8295c7f25b5",
       "IPY_MODEL_87df2842719045488595c9725e386297",
       "IPY_MODEL_6de2a2c502be4e4fa4770ea0eb4fe94e"
      ],
      "layout": "IPY_MODEL_85e055a4dcaf4131ad4cad24b5b2410c"
     }
    },
    "ed9c813e80fc43b096bda875f665b176": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
