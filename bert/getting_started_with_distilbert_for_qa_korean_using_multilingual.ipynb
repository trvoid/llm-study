{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/trvoid/llm-study/blob/main/bert/getting_started_with_distilbert_for_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cvu_6JOiK0j"
   },
   "source": [
    "# DistilBERT for QA 시작하기 (한국어)\n",
    "\n",
    "**노트: distilbert-base-multilingual-cased를 기본 모델로 사용하였음**\n",
    "\n",
    "이 실습은 아래 문서의 내용을 토대로 진행하였습니다.\n",
    "\n",
    "* [Fine-Tuning DistilBERT for Question Answering](https://machinelearningmastery.com/fine-tuning-distilbert-for-question-answering/), By Muhammad Asad Iqbal Khan on March 29, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4PMWlGHibc1"
   },
   "source": [
    "## 1. 해결하고자 하는 문제\n",
    "\n",
    "다음과 같은 유형의 문제를 언어 모델을 사용하여 풀고자 합니다.\n",
    "\n",
    "* 지문(context): 고양이가 의자에 앉습니다.\n",
    "* 질문(question): 고양이가 어디에 앉습니까?\n",
    "* 답변(answer): 의자 (지문에서 첫번째 문자의 위치를 0이라고 할 때 답변의 시작 위치는 5, 끝 위치는 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WekkNEWOh0OC",
    "outputId": "3952d28b-0f0c-4b2d-da72-dd0aae3d5782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_char=5, end_char=7\n"
     ]
    }
   ],
   "source": [
    "context = \"고양이가 의자에 앉습니다.\"\n",
    "answer = \"의자\"\n",
    "\n",
    "start_char = context.find(answer)\n",
    "end_char = start_char + len(answer)\n",
    "print(f\"start_char={start_char}, end_char={end_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YerIbsCNm9wH"
   },
   "source": [
    "주어진 지문으로부터 질문에 답변하는 방식에 따라 아래 두 가지로 구분할 수 있습니다.\n",
    "\n",
    "* 추출형 질의응답 (Extractive Question Answering): 주어진 지문 내에서 질문에 대한 답변에 해당하는 부분을 그대로 찾아 추출하는 방식\n",
    "* 생성형 질의응답 (Abstractive Question Answering): 주어진 지문의 내용을 이해하고 요약하거나 재구성하여 질문에 대한 답변을 새롭게 생성하는 방식\n",
    "\n",
    "여기서 다루는 문제는 추출형 질의응답에 해당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLRK87J4oijH"
   },
   "source": [
    "## 2. 문제 해결 방안\n",
    "\n",
    "언어 모델 훈련에 사용할 데이터를 아래의 형식에 맞추어 준비합니다.\n",
    "\n",
    "* 입력 데이터: 질문과 지문을 토큰화하고 이어붙인 배열\n",
    "  * \"[CLS] *question* [SEP] *context* [SEP]\"\n",
    "* 정답 데이터: 입력 데이터에서 답변의 시작 토큰과 끝 토큰의 위치 정보\n",
    "  * 입력 데이터에서 답변의 시작 토큰 위치\n",
    "  * 입력 데이터에서 답변의 끝 토큰 위치\n",
    "\n",
    "위 형식의 데이터를 대상으로 훈련하는 과정은 다음과 같습니다.\n",
    "\n",
    "* 모델과 데이터셋 선택\n",
    "  * 언어 이해를 목적으로 훈련된 기본 모델 선택\n",
    "  * 질의응답 작업을 위해 미세조정 훈련을 수행할 모델 선택\n",
    "  * 질의응답 미세조정 훈련에 사용할 데이터셋 선택\n",
    "* 미세조정 훈련\n",
    "  1. 미세조정 모델로 정답을 예측하고 손실 계산\n",
    "  2. 미세조정 모델 파라미터 갱신\n",
    "\n",
    "이 실습에서 사용할 모델과 데이터셋은 다음과 같습니다.\n",
    "\n",
    "1. [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert):기본 모델 (단어에 대한 단순 임베딩이 아니라 맥락을 고려한 임베딩 수행)\n",
    "2. [DistilBertForQuestionAnswering](https://huggingface.co/docs/transformers/en/model_doc/distilbert?usage=Pipeline#transformers.DistilBertForQuestionAnswering): DistilBERT 모델에 질의응답 층을 추가한 것으로서 질의응답 미세조정 훈련을 위한 모델\n",
    "3. [KorQuAD 1.0](https://huggingface.co/datasets/KorQuAD/squad_kor_v1): 질의응답 미세조정 훈련을 위한 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wjt9JTaBkod"
   },
   "source": [
    "## 3. 데이터셋 적재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDwsB6s4lTMM"
   },
   "source": [
    "`datasets` 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oB02kkoqvW08",
    "outputId": "30176221-d22f-403c-9ae8-677cd8996e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (0.26.0)\n",
      "Requirement already satisfied: packaging in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from aiohttp->datasets) (1.15.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPqrfHggldKK"
   },
   "source": [
    "KorQuAD 데이터셋을 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jT4hoDawu1Sx"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the KorQuAD dataset\n",
    "dataset = load_dataset(\"KorQuAD/squad_kor_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtE0YtRFk6md"
   },
   "source": [
    "`dataset`의 구조를 출력해서 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bl-pxd78vrij",
    "outputId": "e20632a3-d9a9-437c-9320-810dfde54b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 60407\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 5774\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSb5USfCkbPl"
   },
   "source": [
    "\"train\" 항목의 첫번째 데이터를 출력해서 실제로 어떤 값을 가지고 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yza3i4GpIFzP",
    "outputId": "0e55d4f4-b409-44cc-e38d-2d45d75d1684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"6566495-0-0\",\n",
      "    \"title\": \"파우스트_서곡\",\n",
      "    \"context\": \"1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\",\n",
      "    \"question\": \"바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\",\n",
      "    \"answers\": {\n",
      "        \"text\": [\n",
      "            \"교향곡\"\n",
      "        ],\n",
      "        \"answer_start\": [\n",
      "            54\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(dataset[\"train\"][0], ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuvzgR2fC3-C"
   },
   "source": [
    "## 4. 토크나이저와 모델 적재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5mFZO1ZmAnr"
   },
   "source": [
    "`transformers` 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "poX7Xe7pEOso",
    "outputId": "cc737986-a986-4e65-a0fa-68b92b03753f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo6UslWcmIpO"
   },
   "source": [
    "기본 모델 `distilbert-base-multilingual-cased`를 지정하고 `DistilBertTokenizerFast` 클래스를 사용하여 토크나이저를, `DistilBertForQuestionAnswering` 클래스를 사용하여 모델을 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zADW5Gsu-Gb",
    "outputId": "c9740f3f-c8cf-43cc-aae3-587b9d282916"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 13:06:37.571016: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-07 13:06:37.578499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743998797.586394 2426081 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743998797.588796 2426081 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743998797.595986 2426081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743998797.595993 2426081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743998797.595994 2426081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743998797.595995 2426081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-07 13:06:37.598619: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "\n",
    "# Load tokenizer and model\n",
    "base_model_name = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(base_model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2H3lSaUFL81"
   },
   "source": [
    "## 5. 토크나이저 테스트\n",
    "\n",
    "훈련 데이터 준비 과정에 대한 이해를 돕기 위하여 간단한 예제로 토크나이저를 테스트해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O4PgV3IaEoVt"
   },
   "outputs": [],
   "source": [
    "questions = [\"고양이가 어디에 앉습니까?\"]\n",
    "contexts = [\"고양이가 의자에 앉습니다.\"]\n",
    "\n",
    "inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=25,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbXDTqCZpKrg"
   },
   "source": [
    "토크나이저 실행 결과인 `inputs`의 유형과 키 항목들을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrIII6NIpRwa",
    "outputId": "06fffcfa-8cc2-4220-f607-437214d7a379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'attention_mask', 'offset_mapping'])\n"
     ]
    }
   ],
   "source": [
    "print(type(inputs))\n",
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HioUORAqFN-"
   },
   "source": [
    "각각의 키 항목들에 해당하는 값을 출력하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYS25q6opxHd",
    "outputId": "493b2f82-85ef-4441-9ec6-86d2ffa06a78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 8888, 37114, 57362, 9546, 48446, 10530, 9522, 119081, 25503, 118671, 136, 102, 8888, 37114, 57362, 9637, 13764, 10530, 9522, 119081, 48345, 119, 102, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]\n",
      "offset_mapping: [[(0, 0), (0, 1), (1, 2), (2, 4), (5, 6), (6, 7), (7, 8), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (0, 0), (0, 1), (1, 2), (2, 4), (5, 6), (6, 7), (7, 8), (9, 10), (10, 11), (11, 13), (13, 14), (0, 0), (0, 0)]]\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids: \", end=\"\")\n",
    "print(inputs[\"input_ids\"])\n",
    "\n",
    "print(\"attention_mask: \", end=\"\")\n",
    "print(inputs[\"attention_mask\"])\n",
    "\n",
    "print(\"offset_mapping: \", end=\"\")\n",
    "print(inputs[\"offset_mapping\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a81S7uWiqPpj"
   },
   "source": [
    "`input_ids`에 해당하는 값을 토큰 문자열로 변환해서 보면 각각의 키 항목들의 의미를 좀 더 쉽게 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5UmQXnyFwhf",
    "outputId": "2beee27a-a4ff-4b36-e4bb-912bd3a9aa19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_strs: ['[CLS]', '고', '##양', '##이가', '어', '##디', '##에', '앉', '##습', '##니', '##까', '?', '[SEP]', '고', '##양', '##이가', '의', '##자', '##에', '앉', '##습', '##니다', '.', '[SEP]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "token_strs = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "print(\"token_strs: \", end=\"\")\n",
    "print(token_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZB_TDAzq7fz"
   },
   "source": [
    "`token_strs`을 살펴 보면 답변에 해당하는 토큰 'bench'의 시작 위치와 끝 위치가  각각 11임을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI6-4E_lK9UN"
   },
   "source": [
    "## 6. 토큰 배열에서 답변의 시작 토큰 위치와 끝 토큰 위치 찾는 방법\n",
    "\n",
    "토크나이저 출력 결과 중에서 활용해야 할 정보가 하나 더 있는데 그것은 sequence_ids입니다. 이것으로부터 각각의 토큰이 토크나이저에 입력한 문자열들 중에서 몇 번째 문자열에 해당하는 것인지 파악할 수 있습니다.\n",
    "\n",
    "* sequence_ids의 값이 0이면 토크나이저의 첫번째 인자(`questions`)로부터 온 토큰\n",
    "* sequence_ids의 값이 1이면 토크나이저의 두번째 인자(`contexts`)로부터 온 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GizydGWep4-2",
    "outputId": "8e755e94-b887-4471-c045-739a55f6c4d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(sequence_ids): <class 'method'>\n",
      "sequence_ids(0): [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(\"type(sequence_ids): \", end=\"\")\n",
    "print(type(inputs.sequence_ids))\n",
    "\n",
    "print(\"sequence_ids(0): \", end=\"\")\n",
    "print(inputs.sequence_ids(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKCMy2scrHxf"
   },
   "source": [
    "이제 토큰 배열에서 답변의 시작 토큰 위치와 끝 토큰 위치를 찾기 위해 필요로 하는 정보는 모두 확보하였습니다.\n",
    "\n",
    "* `inputs.sequence_ids(0): [None, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, None, None]`\n",
    "* `inputs[\"offset_mapping\"][0]: [(0, 0), (0, 5), (6, 10), (11, 14), (15, 19), (19, 20), (0, 0), (0, 3), (4, 8), (9, 11), (12, 13), (14, 19), (19, 20), (0, 0), (0, 0)]`\n",
    "* `start_char: 14`\n",
    "* `end_char: 19`\n",
    "\n",
    "이 정보들을 활용하여 답변의 위치를 기계적으로 찾는 과정은 다음과 같습니다.\n",
    "\n",
    "1. sequence_ids에서 context에 해당하는 토큰들 위치 찾기\n",
    "  1. 값이 1인 요소들 중에서 첫번째 것의 위치: `7`\n",
    "  2. 값이 1인 요소들 중에서 마지막 것의 위치: `12`\n",
    "2. offset_mapping에서 context에 해당하는 토큰들을 대상으로 답변의 시작 위치와 끝 위치에 해당하는 토큰 위치 찾기\n",
    "  1. `start_char` 값인 14를 포함하는 offset_mapping 튜플의 위치: `11`\n",
    "  2. `end_char` 값인 19를 포함하는 offset_mapping 튜플의 위치: `11`\n",
    "\n",
    "답변의 시작 토큰 위치와 끝 토큰의 위치가 훈련 데이터에서 입력 데이터에 대한 정답으로 간주되는 값입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiXcWOvnTjn8"
   },
   "source": [
    "## 7. 훈련 데이터 생성\n",
    "\n",
    "원본 데이터셋에서 개별 데이터 항목은 아래와 같은 구조를 가집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "G8hKTEhUwnct"
   },
   "outputs": [],
   "source": [
    "examples_like_squad = [\n",
    "    {\n",
    "        \"context\": \"고양이가 의자에 앉습니다.\",\n",
    "        \"question\": \"고양이가 어디에 앉습니까?\",\n",
    "        \"answers\": {\n",
    "            \"text\": [\"의자\"],\n",
    "            \"answer_start\": [5]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf61oon22l5y"
   },
   "source": [
    "훈련용 데이터 형식으로 변환하는 전처리 함수는 아래와 같은 구조의 데이터를 인자로 받아서 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LdjcAFdP0PpC"
   },
   "outputs": [],
   "source": [
    "examples_for_preprocess = {\n",
    "    \"context\": [\"고양이가 의자에 앉습니다.\"],\n",
    "    \"question\": [\"고양이가 어디에 앉습니까?\"],\n",
    "    \"answers\": [\n",
    "        {\n",
    "            \"text\": [\"의자\"],\n",
    "            \"answer_start\": [5]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7VZuSSI3hyP"
   },
   "source": [
    "데이터셋을 입력으로 받아서 토큰 배열로 만들고 답변의 시작 토큰 위치와 끝 토큰의 위치를 찾아서 훈련용 데이터를 만드는 전처리 함수의 구현은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2r1EwGLZvFAt"
   },
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def preprocess_function(examples, max_length=384):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offsets[context_start][0] > end_char or offsets[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise find the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offsets[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offsets[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGFucTtw3-Aq"
   },
   "source": [
    "위에서 준비한 테스트용 데이터셋을 사용하여 전처리 함수를 실행해 봅니다. 화면 표시의 편의를 위하여 `max_length`의 값을 15로 지정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHGyiwPaUKZU",
    "outputId": "7bb63427-7e0f-4993-c6ef-f788a9779ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 8888, 37114, 57362, 9546, 48446, 10530, 9522, 119081, 25503, 118671, 136, 102, 8888, 37114, 57362, 9637, 13764, 10530, 9522, 119081, 48345, 119, 102, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]\n",
      "start_positions: [16]\n",
      "end_positions: [17]\n"
     ]
    }
   ],
   "source": [
    "results = preprocess_function(examples_for_preprocess, max_length=25)\n",
    "print(f'input_ids: {results[\"input_ids\"]}')\n",
    "print(f'attention_mask: {results[\"attention_mask\"]}')\n",
    "print(f'start_positions: {results[\"start_positions\"]}')\n",
    "print(f'end_positions: {results[\"end_positions\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d2345c97cf9e4f4a9c1a1344c928fdf0",
      "a4088c8d15fb41efb56ca8295c7f25b5",
      "87df2842719045488595c9725e386297",
      "6de2a2c502be4e4fa4770ea0eb4fe94e",
      "85e055a4dcaf4131ad4cad24b5b2410c",
      "32fa3ff8af104c4d9f23ed652c721e20",
      "30d72eedc6714e21a7f200cb0abcfd33",
      "ed9c813e80fc43b096bda875f665b176",
      "1ade48e3bc6d4a46a86b599e87ea406b",
      "8eabe883b07b4d968dd2cd8bff9ca264",
      "6de4057a54f84d10a3dcc7e0535a86e5"
     ]
    },
    "id": "GqJYhmQOT4Vc",
    "outputId": "7ae35b34-aef5-463d-cb21-5aa9cd97a4bf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a700ee69dd42bbb7cc4917dba5f1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply preprocessing to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function,\n",
    "                                 batched=True,\n",
    "                                 remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYA38CqSWOYM"
   },
   "source": [
    "## 8. 미세조정 훈련\n",
    "\n",
    "이제 SQuAD 데이터셋을 대상으로 DistilBertForQuestionAnswering 모델을 훈련합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EZmfn1Afm36",
    "outputId": "77251fbf-6eb2-4883-d3ec-13a4e76b841a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: accelerate in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (0.26.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: requests in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wjeong/DevEnv/py312/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "jlTnf83-T_xf",
    "outputId": "472dbaf4-5e88-4834-cec6-147aa9e3b320"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjeong/DevEnv/py312/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_2426081/173197551.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11328' max='11328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11328/11328 14:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.951200</td>\n",
       "      <td>0.827501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.725673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.514500</td>\n",
       "      <td>0.739571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-distilbert-korquad/tokenizer_config.json',\n",
       " './fine-tuned-distilbert-korquad/special_tokens_map.json',\n",
       " './fine-tuned-distilbert-korquad/vocab.txt',\n",
       " './fine-tuned-distilbert-korquad/added_tokens.json',\n",
       " './fine-tuned-distilbert-korquad/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model and save the results\n",
    "trainer.train()\n",
    "\n",
    "finetuned_model_path = \"./fine-tuned-distilbert-korquad\"\n",
    "model.save_pretrained(finetuned_model_path)\n",
    "tokenizer.save_pretrained(finetuned_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZdlfDhR5Lj-"
   },
   "source": [
    "## 9. 미세조정 훈련 모델로 질의응답 수행\n",
    "\n",
    "지문과 질문을 인자로 받아서 답변을 찾아 반환하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JwbCfrNC5bAL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_answer(tokenizer, model, context, question):\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    print(f\"input_tokens: {input_tokens}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    output_tokens = tokenizer.convert_ids_to_tokens(predict_answer_tokens)\n",
    "\n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmJKDKhkYncw"
   },
   "source": [
    "먼저 질의응답 훈련을 하지 않은 기본 모델로 질의응답을 수행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mTL_JdURYFCU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens: ['[CLS]', '철', '##수는', '어', '##디', '##에', '삽', '##니', '##까', '?', '[SEP]', '철', '##수는', '서울', '##에', '삽', '##니다', '.', '[SEP]']\n",
      "answer by base model: ['?', '[SEP]', '철', '##수는', '서울', '##에', '삽']\n"
     ]
    }
   ],
   "source": [
    "test_context = \"철수는 서울에 삽니다.\"\n",
    "test_question = \"철수는 어디에 삽니까?\"\n",
    "\n",
    "base_tokenizer = DistilBertTokenizerFast.from_pretrained(base_model_name)\n",
    "base_model = DistilBertForQuestionAnswering.from_pretrained(base_model_name)\n",
    "\n",
    "test_output_tokens = predict_answer(base_tokenizer, base_model, test_context, test_question)\n",
    "print(f\"answer by base model: {test_output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74uJn51vYvbU"
   },
   "source": [
    "이제 질의응답 데이터에 대하여 미세조정 훈련을 거친 모델로 답변을 예측하고 기본 모델의 예측 결과와 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jbO3_1mPYbf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens: ['[CLS]', '철', '##수는', '어', '##디', '##에', '삽', '##니', '##까', '?', '[SEP]', '철', '##수는', '서울', '##에', '삽', '##니다', '.', '[SEP]']\n",
      "answer by finetuned model: ['서울']\n"
     ]
    }
   ],
   "source": [
    "finetuned_tokenizer = DistilBertTokenizerFast.from_pretrained(finetuned_model_path)\n",
    "finetuned_model = DistilBertForQuestionAnswering.from_pretrained(finetuned_model_path)\n",
    "\n",
    "test_output_tokens = predict_answer(finetuned_tokenizer, finetuned_model, test_context, test_question)\n",
    "print(f\"answer by finetuned model: {test_output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답변이 기대한 바와 일치하는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZ3y78jzXbpSmLBC0mDN3f",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ade48e3bc6d4a46a86b599e87ea406b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30d72eedc6714e21a7f200cb0abcfd33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32fa3ff8af104c4d9f23ed652c721e20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de2a2c502be4e4fa4770ea0eb4fe94e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8eabe883b07b4d968dd2cd8bff9ca264",
      "placeholder": "​",
      "style": "IPY_MODEL_6de4057a54f84d10a3dcc7e0535a86e5",
      "value": " 10570/10570 [00:15&lt;00:00, 788.93 examples/s]"
     }
    },
    "6de4057a54f84d10a3dcc7e0535a86e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85e055a4dcaf4131ad4cad24b5b2410c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87df2842719045488595c9725e386297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed9c813e80fc43b096bda875f665b176",
      "max": 10570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ade48e3bc6d4a46a86b599e87ea406b",
      "value": 10570
     }
    },
    "8eabe883b07b4d968dd2cd8bff9ca264": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4088c8d15fb41efb56ca8295c7f25b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32fa3ff8af104c4d9f23ed652c721e20",
      "placeholder": "​",
      "style": "IPY_MODEL_30d72eedc6714e21a7f200cb0abcfd33",
      "value": "Map: 100%"
     }
    },
    "d2345c97cf9e4f4a9c1a1344c928fdf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4088c8d15fb41efb56ca8295c7f25b5",
       "IPY_MODEL_87df2842719045488595c9725e386297",
       "IPY_MODEL_6de2a2c502be4e4fa4770ea0eb4fe94e"
      ],
      "layout": "IPY_MODEL_85e055a4dcaf4131ad4cad24b5b2410c"
     }
    },
    "ed9c813e80fc43b096bda875f665b176": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
